{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2W3O2fOFB-4"
      },
      "source": [
        "## A continuous formulation for the Maximum s-defective Clique Problem in k-graphs\n",
        "\n",
        "Let $G := (V, E)$ be a $k$-graph with vertices $V$ and edges $E$. Let $0 < \\alpha \\leq \\frac{1}{k(k - 1)}$ (with strict inequality for $k = 2$) and $\\beta > 0$ and consider the following problem :\n",
        "\\begin{equation}\n",
        "    \\max_{\\text{s.t. } (x,y) \\in \\mathcal{P}_s} \\sum_{e \\in \\overline{E}} y_e \\prod_{i \\in e} x_i - \\sum_{e \\in \\overline{E}} \\prod_{i \\in e} x_i - \\alpha \\|x\\|_k^k + \\beta \\|y\\|_2^2\n",
        "\\end{equation}\n",
        "\n",
        "In the following, we will denote $\\tilde{h}$ the objective of the above problem. We claim that maximizers of this problem are attained at $p = (x^{(C)}, y^{(p)})$, where $s \\geq l = \\textbf{1}^Ty^{(p)} \\in \\mathbb{N}$, with $C$ an $l$-defective clique in $G$ which is also a maximal clique in $G\\cup G(y^{(p)})$, and $y^{(p)} \\in \\{0, 1\\}^{\\overline{E}}$ such that $y_e^{(p)} = 1$ for every $e \\in \\binom{C}{k}\\cap \\overline{E}$ and with $\\text{supp}(y^{(p)})$ of maximum cardinality under these constraints, and conversely we also claim that every maximal $s$-defective clique in $G$ is associated to such a minimizer.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfK7rGJE-Isj"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oc8EzBPCPWpw"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from matplotlib.patches import Polygon\n",
        "from matplotlib.collections import PatchCollection\n",
        "from matplotlib.lines import Line2D\n",
        "import scipy as sp\n",
        "import scipy.optimize\n",
        "from itertools import permutations, combinations, product\n",
        "from IPython.display import clear_output\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "import torch\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "COLAB = True # Disable if not on Colab, used to address a bug with\n",
        "             # inplace plotting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CxjZLot-Sj_"
      },
      "source": [
        "# Functions on vertices and hypergraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LrtRtcmBQC_R"
      },
      "outputs": [],
      "source": [
        "def generate_random_adjacency_tensor(k, n, prob):\n",
        "    \"\"\"\n",
        "    Generate a random binary adjacency tensor for a hypergraph.\n",
        "\n",
        "    Parameters:\n",
        "    k (int): The rank of the hypergraph (number of vertices in each edge).\n",
        "    n (int): The number of vertices.\n",
        "    prob (float): The probability for each edge to be part of the graph.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: A random binary adjacency tensor of shape (n, n, ..., n).\n",
        "    \"\"\"\n",
        "    # Create a tensor filled with zeros\n",
        "    adjacency_tensor = torch.zeros((n,) * k, dtype=torch.float64)\n",
        "\n",
        "    # Generate random unique edges\n",
        "    for idx in combinations(range(n), k):\n",
        "        # Assign an edge with probability prob\n",
        "        if np.random.rand() < prob:\n",
        "            adjacency_tensor[tuple(zip(*permutations(idx)))] = 1\n",
        "\n",
        "    return adjacency_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ieyOPlJCElB8"
      },
      "outputs": [],
      "source": [
        "def complementary_hypergraph(adjacency_tensor):\n",
        "    \"\"\"\n",
        "    Compute the adjacency tensor of the complementary hypergraph.\n",
        "\n",
        "    Parameters:\n",
        "    adjacency_tensor (torch.Tensor): The adjacency tensor of the original hypergraph.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The adjacency tensor of the complementary hypergraph.\n",
        "    \"\"\"\n",
        "    n = adjacency_tensor.shape[0]\n",
        "    k = len(adjacency_tensor.shape)\n",
        "\n",
        "    # Initialize with one on all indices coding for edges\n",
        "    complementary_adjacency_tensor = torch.zeros(adjacency_tensor.shape, dtype=torch.float64)\n",
        "    complementary_adjacency_tensor[tuple(zip(*permutations(range(n), k)))] = 1\n",
        "\n",
        "    # Reverse the values\n",
        "    return complementary_adjacency_tensor - adjacency_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DLRDxAk_W1au"
      },
      "outputs": [],
      "source": [
        "def is_s_defective_clique(adjacency_tensor, x, s):\n",
        "    \"\"\"\n",
        "    Check if the support of vector x forms an s-defective k-clique in a hypergraph.\n",
        "\n",
        "    Parameters:\n",
        "    adjacency_tensor (torch.Tensor): The adjacency tensor of the hypergraph.\n",
        "    x (torch.Tensor): The vector representing chosen vertices.\n",
        "    s (int): The maximum number of missing edges.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if the support of x forms an s-defective k-clique, False otherwise.\n",
        "    \"\"\"\n",
        "    k = len(adjacency_tensor.shape)\n",
        "\n",
        "    # Find the support indices (indices where x is non-zero)\n",
        "    support_indices = torch.nonzero(x).squeeze()\n",
        "\n",
        "    # Check if there are at least k vertices in the support\n",
        "    if len(support_indices) < k:\n",
        "        return False\n",
        "\n",
        "    # Check if there are less than s missing edges\n",
        "    missing_edges = 0\n",
        "\n",
        "    # Works faster than torch.sum < s\n",
        "    for idx in combinations(support_indices, k):\n",
        "        if adjacency_tensor[idx] == 0:\n",
        "            missing_edges += 1\n",
        "\n",
        "            #Skip useless computations\n",
        "            if missing_edges > s:\n",
        "                return False\n",
        "\n",
        "    # Get to this point only if the number of missing edges is less than or equal to s\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_aXxKHoXeMIV"
      },
      "outputs": [],
      "source": [
        "def plot_hypergraph(adjacency_tensor, vertices_positions):\n",
        "    \"\"\"\n",
        "    Plot a hypergraph using an adjacency tensor and vertices positions.\n",
        "\n",
        "    Parameters:\n",
        "    adjacency_tensor (torch.Tensor): The binary adjacency tensor.\n",
        "    vertices_positions (ndarray): The positions of vertices on a plane, size (2, n).\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    n = adjacency_tensor.shape[0]\n",
        "    k = len(adjacency_tensor.shape)\n",
        "\n",
        "    # Create a figure and axis\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Find the edges to plot\n",
        "    indices_edge = torch.nonzero(adjacency_tensor)\n",
        "    edges = map(tuple, indices_edge)\n",
        "\n",
        "    # Plot edges as transparent polygons\n",
        "    for edge_idx in edges:\n",
        "        edge_points = vertices_positions[:, edge_idx]\n",
        "        edge_polygon = mpatches.Polygon(edge_points.T, closed=True, fill=True, edgecolor='b', alpha=0.2)\n",
        "        ax.add_patch(edge_polygon)\n",
        "\n",
        "    # Plot vertices as points\n",
        "    ax.scatter(vertices_positions[0], vertices_positions[1])\n",
        "\n",
        "    # Set axis limits and labels\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_aspect('equal', adjustable='box')\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HN7VijpKrJUl"
      },
      "outputs": [],
      "source": [
        "def plot_fake_edges_and_final_clique(adjacency_tensor, vertices_positions, x, y, subplot=None):\n",
        "    \"\"\"\n",
        "    Plot the fake edges in the hypergraph and the final clique using an adjacency tensor and vertices positions.\n",
        "\n",
        "    Parameters:\n",
        "    adjacency_tensor (torch.Tensor): The binary adjacency tensor.\n",
        "    vertices_positions (ndarray): The positions of vertices on a plane, size (2, n).\n",
        "    x (torch.Tensor): The vector x coding for the final clique.\n",
        "    y (torch.Tensor): The multidimensional tensor y representing the fake edges.\n",
        "    subplot (matplotlib.axes._subplots.AxesSubplot) : The subplot to work with, if set to None then a new one is created.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    n = adjacency_tensor.shape[0]\n",
        "    k = len(adjacency_tensor.shape)\n",
        "\n",
        "    # Create a figure and axis\n",
        "    if subplot == None:\n",
        "      fig, ax = plt.subplots()\n",
        "    else:\n",
        "      fig, ax = subplot\n",
        "\n",
        "    # Find the edges to plot\n",
        "    edges_original_graph = []\n",
        "    support_x = torch.nonzero(x).squeeze()\n",
        "\n",
        "    for idx in combinations(support_x, k):\n",
        "        if adjacency_tensor[idx] == 1:\n",
        "            edges_original_graph.append(idx)\n",
        "\n",
        "    fake_edges = map(tuple, torch.nonzero(y))\n",
        "\n",
        "    # Plot edges as transparent polygons\n",
        "    for edge_idx in edges_original_graph:\n",
        "        edge_points = vertices_positions[:, torch.tensor(edge_idx)]\n",
        "        edge_polygon = mpatches.Polygon(edge_points.T, closed=True, fill=True, edgecolor='b', facecolor='b',alpha=0.2)\n",
        "        ax.add_patch(edge_polygon)\n",
        "\n",
        "    for edge_idx in fake_edges:\n",
        "        edge_points = vertices_positions[:, torch.tensor(edge_idx)]\n",
        "        edge_polygon = mpatches.Polygon(edge_points.T, closed=True, fill=True, edgecolor='r', facecolor='r',alpha=0.2*y[tuple(torch.tensor(edge_idx))].item())\n",
        "        ax.add_patch(edge_polygon)\n",
        "\n",
        "    # Plot vertices as points\n",
        "    ax.scatter(vertices_positions[0, :], vertices_positions[1, :], edgecolors='b', alpha = 0.2*(1 - x/torch.max(x))) #Unchosen vertices\n",
        "    ax.scatter(vertices_positions[0, :], vertices_positions[1, :], edgecolors='g', facecolors='y', alpha = 1*x/torch.max(x).item()) #Chosen vertices\n",
        "\n",
        "    # Add a legend\n",
        "    green_dot = Line2D([0], [0], marker='o', markeredgecolor='b', linestyle='', alpha=0.2, label='Unchosen vertices')\n",
        "    orange_dot = Line2D([0], [0], marker='o', markeredgecolor='g', markerfacecolor='y', linestyle='', alpha=1, label='Vertices of the clique')\n",
        "\n",
        "    blue_patch = mpatches.Patch(color='b', label='Edges of the original graph')\n",
        "    red_patch = mpatches.Patch(color='r', label='Fake edges')\n",
        "\n",
        "    handles = [green_dot, orange_dot, blue_patch, red_patch]\n",
        "\n",
        "    ax.legend(handles=handles, loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8GWG1hZCHT-"
      },
      "source": [
        "# DIMACS dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjKw8oL3Cba4"
      },
      "source": [
        "Download 2-graphs from the [DIMACS dataset](https://iridia.ulb.ac.be/~fmascia/maximum_clique/DIMACS-benchmark), convert them to k-graphs and initialize a dictionary containing the best known cliques sizes.\n",
        "\n",
        "Available graphs are :\n",
        "- C125.9\n",
        "- C250.9\n",
        "- C500.9\n",
        "- C1000.9\n",
        "- DSJC1000_5\n",
        "- DSJC500_5\n",
        "- MANN_a27\n",
        "- MANN_a45\n",
        "- brock200_2\n",
        "- brock200_4\n",
        "- brock400_2\n",
        "- brock400_4\n",
        "- brock800_2\n",
        "- brock800_4\n",
        "- gen200_p0.9_44\n",
        "- gen200_p0.9_55\n",
        "- gen400_p0.9_55\n",
        "- gen400_p0.9_65\n",
        "- gen400_p0.9_75\n",
        "- hamming10-4\n",
        "- hamming8-4\n",
        "- keller4\n",
        "- keller5\n",
        "- p_hat300-1\n",
        "- p_hat300-2\n",
        "- p_hat300-3\n",
        "- p_hat700-1\n",
        "- p_hat700-2\n",
        "- p_hat700-3\n",
        "- p_hat1500-1\n",
        "- p_hat1500-2\n",
        "- p_hat1500-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kk2URduHT9mj"
      },
      "outputs": [],
      "source": [
        "# Initialize a dictionary containing the best known cliques sizes on each graph\n",
        "best_known_cliques_sizes = {\n",
        "    \"C125.9\": 34,\n",
        "    \"C250.9\": 44,\n",
        "    \"C500.9\": 57,\n",
        "    \"C1000.9\": 68,\n",
        "    \"C2000.9\": 80,\n",
        "    \"DSJC1000_5\": 15,\n",
        "    \"DSJC500_5\": 13,\n",
        "    \"C2000.5\": 16,\n",
        "    \"C4000.5\": 18,\n",
        "    \"MANN_a27\": 126,\n",
        "    \"MANN_a45\": 345,\n",
        "    \"MANN_a81\": 1100,\n",
        "    \"brock200_2\": 12,\n",
        "    \"brock200_4\": 17,\n",
        "    \"brock400_2\": 29,\n",
        "    \"brock400_4\": 33,\n",
        "    \"brock800_2\": 24,\n",
        "    \"brock800_4\": 26,\n",
        "    \"gen200_p0.9_44\": 44,\n",
        "    \"gen200_p0.9_55\": 55,\n",
        "    \"gen400_p0.9_55\": 55,\n",
        "    \"gen400_p0.9_65\": 65,\n",
        "    \"gen400_p0.9_75\": 75,\n",
        "    \"hamming10-4\": 40,\n",
        "    \"hamming8-4\": 16,\n",
        "    \"keller4\": 11,\n",
        "    \"keller5\": 27,\n",
        "    \"keller6\": 59,\n",
        "    \"p_hat300-1\": 8,\n",
        "    \"p_hat300-2\": 25,\n",
        "    \"p_hat300-3\": 36,\n",
        "    \"p_hat700-1\": 11,\n",
        "    \"p_hat700-2\": 44,\n",
        "    \"p_hat700-3\": 62,\n",
        "    \"p_hat1500-1\": 12,\n",
        "    \"p_hat1500-2\": 65,\n",
        "    \"p_hat1500-3\": 94\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XZ7E7U2mFn-H"
      },
      "outputs": [],
      "source": [
        "def read_DIMACS_graph(graph_str):\n",
        "    \"\"\"\n",
        "    Read a DIMACS formatted graph string and generate its adjacency matrix as an ndarray.\n",
        "\n",
        "    Parameters:\n",
        "    graph_str (str): The DIMACS formatted graph string.\n",
        "\n",
        "    Returns:\n",
        "    ndarray: The adjacency matrix of the graph.\n",
        "    \"\"\"\n",
        "    lines = graph_str.split('\\n')\n",
        "\n",
        "    # Initialize variables to store graph information\n",
        "    num_vertices = None\n",
        "    num_edges = None\n",
        "    edges = []\n",
        "\n",
        "    # Parse the graph data\n",
        "    for line in lines:\n",
        "        parts = line.split()\n",
        "        if len(parts) == 0:\n",
        "            continue\n",
        "        if parts[0] == 'p':\n",
        "            num_vertices = int(parts[2])\n",
        "            num_edges = int(parts[3])\n",
        "        elif parts[0] == 'e':\n",
        "            vertex1 = int(parts[1]) - 1  # Adjust indices to start from 0\n",
        "            vertex2 = int(parts[2]) - 1\n",
        "            edges.append((vertex1, vertex2))\n",
        "\n",
        "    # Create an empty adjacency matrix\n",
        "    adjacency_matrix = np.zeros((num_vertices, num_vertices), dtype=int)\n",
        "\n",
        "    # Fill the adjacency matrix based on the edges\n",
        "    for edge in edges:\n",
        "        adjacency_matrix[edge[0], edge[1]] = 1\n",
        "        adjacency_matrix[edge[1], edge[0]] = 1\n",
        "\n",
        "    return adjacency_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tq2hcvxRHFzs"
      },
      "outputs": [],
      "source": [
        "def adjacency_matrix_to_adjacency_tensor(adjacency_matrix, k):\n",
        "    \"\"\"\n",
        "    Convert an adjacency matrix into a symmetric adjacency tensor for k-graphs.\n",
        "\n",
        "    Parameters:\n",
        "    adjacency_matrix (ndarray): The adjacency matrix of the graph.\n",
        "    k (int): The order of the k-graph.\n",
        "\n",
        "    Returns:\n",
        "    ndarray: The symmetric adjacency tensor for k-graphs.\n",
        "    \"\"\"\n",
        "    if k > 2:\n",
        "        n = adjacency_matrix.shape[0]  # Number of vertices\n",
        "        adjacency_tensor = np.zeros((n,) * k, dtype=int)\n",
        "\n",
        "        # Iterate through all possible combinations of k vertices\n",
        "        for indices in combinations(range(n), k):\n",
        "            # Check if there is an edge between all pairs of vertices in the combination\n",
        "            if all(adjacency_matrix[i, j] == 1 for i, j in combinations(indices, 2)):\n",
        "                # Set the corresponding entries in the adjacency tensor to 1\n",
        "                for perm in permutations(indices):\n",
        "                    adjacency_tensor[perm] = 1\n",
        "    else:\n",
        "        return adjacency_matrix\n",
        "\n",
        "    return adjacency_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "AbZpHcf5CGLT"
      },
      "outputs": [],
      "source": [
        "def download_and_convert_DIMACS_graphs(graph_names, k):\n",
        "    \"\"\"\n",
        "    Download DIMACS formatted graph files, convert them into k-graph tensors, and store them in a dictionary.\n",
        "\n",
        "    Parameters:\n",
        "    graph_names (list): A list of strings representing the names of the desired graphs.\n",
        "    k (int): The order of the k-graph to generate.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary where keys are graph names, and values are the k-graph tensors.\n",
        "    \"\"\"\n",
        "    graph_data = {}\n",
        "\n",
        "    base_url = \"http://iridia.ulb.ac.be/~fmascia/files/DIMACS/\"\n",
        "\n",
        "    for graph_name in graph_names:\n",
        "        url = base_url + graph_name + \".clq\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                # Parse the DIMACS file and store the adjacency matrix\n",
        "                adjacency_matrix = read_DIMACS_graph(response.text)\n",
        "\n",
        "                # Generate the associated k-graph tensor\n",
        "                k_graph_tensor = adjacency_matrix_to_adjacency_tensor(adjacency_matrix, k)\n",
        "\n",
        "                graph_data[graph_name] = torch.tensor(k_graph_tensor, dtype=torch.float64)\n",
        "            else:\n",
        "                print(f\"Failed to download {graph_name}.clq\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading {graph_name}.clq: {str(e)}\")\n",
        "\n",
        "    return graph_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA7PNzM7-Yc1"
      },
      "source": [
        "# Miscellaneous functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def s_max_indices(arr, s, k_factorial):\n",
        "    \"\"\"\n",
        "    Get the indices of the s maximal positive values of a symmetrical tensor, ignoring NaN and negative values and considering only the indices corresponding to edges.\n",
        "    Parameters:\n",
        "    arr (torch.Tensor): The input multidimensional tensor.\n",
        "    s (int): The number of maximal indices to return.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "\n",
        "    Returns:\n",
        "    tuple: The indices of the s maximal values.\n",
        "    \"\"\"\n",
        "    n = arr.shape[0]\n",
        "    k = len(arr.shape)\n",
        "\n",
        "    arr_copy = arr.clone()\n",
        "\n",
        "    # Mask out NaN values\n",
        "    mask = torch.isnan(arr_copy)\n",
        "    arr_copy[mask] = float('-inf')\n",
        "\n",
        "    # Get the top s * k_factorial maximal positive values (and not s because the\n",
        "    # tensor is symmetric so all values appear k_factorial times)\n",
        "    top_values, top_indices = torch.topk(arr_copy.flatten(), s * k_factorial, sorted=True)\n",
        "\n",
        "    # Filter out negative and NaN values\n",
        "    valid_indices = top_indices[(top_values > 0)]\n",
        "\n",
        "    # Unravel the flattened indices into multi-dimensional indices\n",
        "    multi_dimensional_indices = torch.unravel_index(valid_indices, arr.shape)\n",
        "\n",
        "    # Filter out the permutation corresponding to the same index\n",
        "    unique_indices = set()\n",
        "    for idx in zip(*multi_dimensional_indices):\n",
        "        sorted_idx = tuple(sorted([item.item() for item in idx])) # the indices have the form (tensor(..), tensor(..))\n",
        "        unique_indices.add(sorted_idx)\n",
        "\n",
        "    if len(unique_indices) > 0:\n",
        "        return tuple(map(list, zip(*list(unique_indices)[:min(s, len(unique_indices))]))) # map(list) to convert the tuples of indices into list, don't know why but very rarely tuples cause a strange bug\n",
        "    else:\n",
        "        return [] # Doing tensor[()] returns the whole tensor and not just an empty one"
      ],
      "metadata": {
        "id": "rt2u38OtxPQw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hBGF7IJmbGck"
      },
      "outputs": [],
      "source": [
        "def s_minimal_indices_in_support(arr1, arr2, s, k_factorial):\n",
        "    \"\"\"\n",
        "    Get the indices of the s minimal values of arr1 that are in the support of arr2.\n",
        "\n",
        "    Parameters:\n",
        "    arr1 (torch.Tensor): The first input multidimensional tensor.\n",
        "    arr2 (torch.Tensor): The second input multidimensional tensor serving as a mask.\n",
        "    s (int): The number of minimal indices to return.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of tuples, each containing the indices of the s minimal values in arr1 based on the mask provided by arr2.\n",
        "    \"\"\"\n",
        "    k = len(arr1.shape)\n",
        "    arr1_copy = arr1.clone()\n",
        "\n",
        "    # Ignore all values outside the mask provided by arr2\n",
        "    arr1_copy[(arr2 == 0)] = float('nan')\n",
        "\n",
        "    # Mask out NaN values\n",
        "    mask = torch.isnan(arr1_copy)\n",
        "    arr1_copy[mask] = float('inf')\n",
        "\n",
        "    # Get the bottom s * k_factorial minimal negative values (and not s because the\n",
        "    # tensor is symmetric so all values appear k_factorial times)\n",
        "    top_values, top_indices = torch.topk(-arr1_copy.flatten(), s * k_factorial, sorted=True)\n",
        "\n",
        "    # Filter out NaN values\n",
        "    valid_indices = top_indices[(top_values != float('-inf'))]\n",
        "\n",
        "    # Unravel the flattened indices into multi-dimensional indices\n",
        "    multi_dimensional_indices = torch.unravel_index(valid_indices, arr1.shape)\n",
        "\n",
        "    # Filter out the permutation corresponding to the same index\n",
        "    unique_indices = set()\n",
        "    for idx in zip(*multi_dimensional_indices):\n",
        "        sorted_idx = tuple(sorted([item.item() for item in idx])) # the indices have the form (tensor(..), tensor(..))\n",
        "        unique_indices.add(sorted_idx)\n",
        "\n",
        "    if len(unique_indices) > 0:\n",
        "        return tuple(map(list, zip(*list(unique_indices)[:min(s, len(unique_indices))]))) # map(list) to convert the tuples of indices into list, don't know why but very rarely tuples cause a strange bug\n",
        "    else:\n",
        "        return [] # Doing tensor[()] returns the whole tensor and not just an empty one\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qv5InhDD-doe"
      },
      "source": [
        "# Objective function and derivatives (novel formulation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvwTYTUV5p1j"
      },
      "source": [
        "Denoting the graph $G = (V, E)$, our objective function is :\n",
        "\\begin{align}\n",
        "  \\tilde{h}(x, y) := -h(x,y) :&= \\sum_{e\\in \\overline{E}}y_e\\prod_{i\\in e}x_i - \\sum_{e\\in \\overline{E}}\\prod_{i\\in e}x_i - \\alpha \\|x\\|_k^k + \\beta \\|y\\|_2^2 \\\\\n",
        "  &= \\frac{1}{k!}(\\mathcal{A}(y) - \\mathcal{A})x^k - \\alpha \\|x\\|_k^k + \\beta \\|y\\|_2^2\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathcal{A}$ is the adjacency tensor of the graph and $\\mathcal{A}(y)$ is the adjacency tensor made of the fake edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C_cukvp-5qK6"
      },
      "outputs": [],
      "source": [
        "def objective_function(Abar, x, y, k, k_factorial, alpha, beta):\n",
        "    \"\"\"\n",
        "    Compute the objective function value using provided tensors and parameters.\n",
        "\n",
        "    Parameters:\n",
        "    Abar (torch.Tensor): The adjacency tensor of the complementary of the graph of shape (n, n, ..., n).\n",
        "    x (torch.Tensor): The vector x of length n.\n",
        "    y (torch.Tensor): The multidimensional tensor y of shape (n, n, ..., n).\n",
        "    k (int): The number of iterations for tensor contraction.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "    alpha (float): The parameter alpha.\n",
        "    beta (float): The parameter beta.\n",
        "\n",
        "    Returns:\n",
        "    float: The computed objective function value.\n",
        "    \"\"\"\n",
        "    # Compute (A(y) - Abar)x^k\n",
        "    Ayxk = y - Abar\n",
        "    for _ in range(k):\n",
        "        Ayxk = torch.tensordot(Ayxk, x, dims=1)\n",
        "\n",
        "    # Compute \\|x\\|_k^k\n",
        "    norm_xk = torch.sum(x ** k)\n",
        "\n",
        "    # Compute \\|y\\|_2^2\n",
        "    norm_y2 = torch.sum(y ** 2)\n",
        "\n",
        "    # Compute the final result\n",
        "    return Ayxk / k_factorial - alpha * norm_xk + beta * norm_y2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQUPbOdg5uXz"
      },
      "source": [
        "The gradient with respect to $x$ is\n",
        "\\begin{align}\n",
        "  \\nabla_x \\tilde{h}(x, y) &= \\frac{k}{k!}(\\mathcal{A}(y) - \\mathcal{A})x^{k-1} - k\\alpha x^{[k-1]}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KAP7a4Dp5upw"
      },
      "outputs": [],
      "source": [
        "def objective_function_gradient_x(Abar, x, y, k, k_factorial, alpha):\n",
        "    \"\"\"\n",
        "    Compute the gradient of the objective function with respect to x.\n",
        "\n",
        "    Parameters:\n",
        "    Abar (torch.Tensor): The adjacency tensor of the complementary of the graph of shape (n, n, ..., n).\n",
        "    x (torch.Tensor): The vector x of length n.\n",
        "    y (torch.Tensor): The multidimensional tensor y of shape (n, n, ..., n).\n",
        "    k (int): The rank of the hypergraph.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "    alpha (float): The parameter alpha.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The computed gradient of the objective function with respect to x.\n",
        "    \"\"\"\n",
        "    # Compute (A(y) - Abar)x^{k-1}\n",
        "    Ayxk_minus_1 = y - Abar\n",
        "    for _ in range(k - 1):\n",
        "        Ayxk_minus_1 = torch.tensordot(Ayxk_minus_1, x, dims=1)\n",
        "\n",
        "    # Compute the final derivative\n",
        "    return Ayxk_minus_1 * k / k_factorial - k * alpha * x**(k - 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7aTb0qc5xNT"
      },
      "source": [
        "The derivative with respect to a component of $y$ is\n",
        "\\begin{align}\n",
        "  \\frac{\\partial \\tilde{h}}{\\partial y_e} (x, y) &= \\prod_{i \\in e}x_i + 2\\beta y_e\n",
        "\\end{align}\n",
        "\n",
        "The full tensor $\\left( \\prod_{i \\in e}x_i + 2\\beta y_e \\right)_e$ is computed to benefit from Numpy's fast implementation of the products (the products tensor is simply $k$ outers products of x), then the edges $e \\in E$ are discarded by replacing their value in the gradient by NaN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PFqLajQQ5ljE"
      },
      "outputs": [],
      "source": [
        "def objective_function_gradient_y(Abar, x, y, k_factorial, beta):\n",
        "    \"\"\"\n",
        "    Compute the gradient of the objective function with respect to y.\n",
        "\n",
        "    Parameters:\n",
        "    Abar (torch.Tensor): The adjacency tensor of the complementary of the graph of shape (n, n, ..., n).\n",
        "    x (torch.Tensor): The vector x of length n.\n",
        "    y (torch.Tensor): The multidimensional tensor y of shape (n, n, ..., n).\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "    beta (float): The parameter beta.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The computed gradient of the objective function with respect to y.\n",
        "    \"\"\"\n",
        "    # Compute the product of the x_i where i is in e\n",
        "    gradient = x\n",
        "    for _ in range(len(Abar.shape) - 1):\n",
        "        gradient = torch.tensordot(gradient, x, dims=0)\n",
        "\n",
        "    gradient += 2 * beta * y # Gradient of the regularization term\n",
        "    gradient[(Abar == 0)] = float('nan')  # The value of edges that do not belong to the complement of the graph are replaced by NaN to discard them\n",
        "\n",
        "    return gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pFV1wy4SbJp"
      },
      "source": [
        "The above function computes the **full** gradient with lots of 0 that will not be used in the FW variants. In practice, in order to avoid computing these expensive but useless values, we only compute a subtensor of the gradient with the following function, which additionally provides an array mapping its indices to the corresponding indices in the full gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "jX_3DpFtTIfM"
      },
      "outputs": [],
      "source": [
        "def objective_function_reduced_gradient_y(Abar, x, y, k_factorial, beta):\n",
        "    \"\"\"\n",
        "    Compute the reduced gradient of the objective function with respect to y.\n",
        "\n",
        "    Parameters:\n",
        "    Abar (torch.Tensor): The adjacency tensor of the complementary graph of shape (n, n, ..., n).\n",
        "    x (torch.Tensor): The vector x of length n.\n",
        "    y (torch.Tensor): The multidimensional tensor y of shape (n, n, ..., n).\n",
        "    k_factorial (int): Preprocessed value of k!.\n",
        "    beta (float): Parameter beta.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Computed reduced gradient of the objective function with respect to y\n",
        "    and a list mapping its indices to the real ones.\n",
        "    \"\"\"\n",
        "    # Find indices contributing to the value 2*beta*y\n",
        "    non_zero_indices_y = torch.nonzero(y != 0, as_tuple=True)\n",
        "    non_zero_indices_x = torch.nonzero(x != 0, as_tuple=True)\n",
        "\n",
        "    # Keep only the indices contributing to non-zero gradient values\n",
        "    non_zero_indices_gradient_set = set(np.concatenate([*non_zero_indices_y, *non_zero_indices_x]))\n",
        "    non_zero_indices_gradient_list = np.sort(list(non_zero_indices_gradient_set)) # Sorted to have an easy correspondence to reconstruct the full gradient\n",
        "\n",
        "    # Compute the reduced arguments\n",
        "    k = len(y.shape)\n",
        "\n",
        "    reduced_indices_y = np.ix_(*[non_zero_indices_gradient_list] * k)\n",
        "    reduced_Abar = Abar[reduced_indices_y]\n",
        "    reduced_y = y[reduced_indices_y]\n",
        "\n",
        "    reduced_x = x[non_zero_indices_gradient_list]\n",
        "\n",
        "    # Compute the reduced gradient for y\n",
        "    return objective_function_gradient_y(reduced_Abar, reduced_x, reduced_y, k_factorial, beta), non_zero_indices_gradient_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo3hORAJ1fqL"
      },
      "source": [
        "# Objective function and derivatives (previous formulation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcg09tZ1TbtS"
      },
      "source": [
        "The following is an implementation of the continuous formulation of the MCP (designed for $k=2$) found in [this paper](https://arxiv.org/abs/2103.15907).\n",
        "\n",
        "Denoting the graph $G = (V, E)$, the previous objective function is :\n",
        "\\begin{align}\n",
        "  h(x, y) := x^T(A + A(y))x + \\frac{\\alpha}{2} \\|x\\|_2^2 + \\frac{\\beta}{2} \\|y\\|_2^2\n",
        "\\end{align}\n",
        "\n",
        "where $A$ is the adjacency matrix of the graph and $A(y)$ is the adjacency matrix made of the fake edges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "daAZfMRp1ee8"
      },
      "outputs": [],
      "source": [
        "def old_objective_function(A, x, y, alpha, beta):\n",
        "    \"\"\"\n",
        "    Compute the objective function value using provided tensors and parameters.\n",
        "\n",
        "    Parameters:\n",
        "    A (torch.Tensor): The adjacency matrix of the graph of shape (n, n).\n",
        "    x (torch.Tensor): The vector x of length n.\n",
        "    y (torch.Tensor): The multidimensional tensor y of shape (n, n).\n",
        "    alpha (float): The parameter alpha.\n",
        "    beta (float): The parameter beta.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The computed objective function value.\n",
        "    \"\"\"\n",
        "    # Compute \\|x\\|_2^2\n",
        "    norm_x2 = torch.sum(x ** 2)\n",
        "\n",
        "    # Compute \\|y\\|_2^2\n",
        "    norm_y2 = torch.sum(y ** 2)\n",
        "\n",
        "    # Compute the final result\n",
        "    return torch.matmul(x, torch.matmul(A + y, x)) + alpha / 2 * norm_x2 + beta / 2 * norm_y2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHcwRXRNVQ3v"
      },
      "source": [
        "The gradient with respect to $x$ is\n",
        "\\begin{align}\n",
        "  \\nabla_x h(x, y) &= 2(A+A(y))x + \\alpha x\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "I60vr4hVTMSr"
      },
      "outputs": [],
      "source": [
        "def old_objective_function_gradient_x(A, x, y, alpha):\n",
        "    \"\"\"\n",
        "    Compute the gradient of the objective function with respect to x.\n",
        "\n",
        "    Parameters:\n",
        "    A (torch.Tensor): The adjacency matrix of the graph of shape (n, n).\n",
        "    x (torch.Tensor): The vector x of length n.\n",
        "    y (torch.Tensor): The multidimensional tensor y of shape (n, n).\n",
        "    alpha (float): The parameter alpha.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The computed gradient of the objective function with respect to x.\n",
        "    \"\"\"\n",
        "    return 2 * torch.matmul(A + y, x) + alpha * x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0Fq2Pd6WDii"
      },
      "source": [
        "The derivative with respect to a component of $y$ is\n",
        "\\begin{align}\n",
        "  \\frac{\\partial h}{\\partial y_{ij}} (x, y) &= 2x_i x_j + \\beta y_{ij}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ETxn8Ol3TUak"
      },
      "outputs": [],
      "source": [
        "def old_objective_function_gradient_y(Abar, x, y, beta):\n",
        "    \"\"\"\n",
        "    Compute the gradient of the objective function with respect to y.\n",
        "\n",
        "    Parameters:\n",
        "    Abar (torch.Tensor): The adjacency matrix of the complementary of the graph of shape (n, n).\n",
        "    x (torch.Tensor): The vector x of length n.\n",
        "    y (torch.Tensor): The multidimensional tensor y of shape (n, n).\n",
        "    beta (float): The parameter beta.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The computed gradient of the objective function with respect to y.\n",
        "    \"\"\"\n",
        "    # Compute x_i*x_j\n",
        "    gradient = 2 * torch.ger(x, x)\n",
        "\n",
        "    gradient += beta * y  # Gradient of the regularization term\n",
        "    gradient[Abar == 0] = float('nan')  # The value of edges that do not belong to the complement of the graph are replaced by NaN to discard them\n",
        "\n",
        "    return gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyGaPaw7aGJI"
      },
      "source": [
        "And its reduced version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "DsE7C9DMaINR"
      },
      "outputs": [],
      "source": [
        "def old_objective_function_reduced_gradient_y(Abar, x, y, beta):\n",
        "    \"\"\"\n",
        "    Compute the reduced gradient of the objective function with respect to y.\n",
        "\n",
        "    Parameters:\n",
        "    Abar (torch.Tensor): The adjacency tensor of the complementary graph of shape (n, n, ..., n).\n",
        "    x (torch.Tensor): The vector x of length n.\n",
        "    y (torch.Tensor): The multidimensional tensor y of shape (n, n, ..., n).\n",
        "    beta (float): Parameter beta.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Computed reduced gradient of the objective function with respect to y\n",
        "    and a list mapping its indices to the real ones.\n",
        "    \"\"\"\n",
        "    # Find indices contributing to the value 2*beta*y\n",
        "    non_zero_indices_y = torch.nonzero(y != 0, as_tuple=True)\n",
        "    non_zero_indices_x = torch.nonzero(x != 0, as_tuple=True)\n",
        "\n",
        "    # Keep only the indices contributing to non-zero gradient values\n",
        "    non_zero_indices_gradient_set = set(np.concatenate([*non_zero_indices_y, *non_zero_indices_x]))\n",
        "    non_zero_indices_gradient_list = np.sort(list(non_zero_indices_gradient_set)) # Sorted to have an easy correspondance to reconstruct the full gradient\n",
        "\n",
        "    # Compute the reduced arguments\n",
        "    k = len(y.shape)\n",
        "\n",
        "    reduced_indices_y = np.ix_(*[non_zero_indices_gradient_list] * k)\n",
        "    reduced_Abar = Abar[reduced_indices_y]\n",
        "    reduced_y = y[reduced_indices_y]\n",
        "\n",
        "    reduced_x = x[non_zero_indices_gradient_list]\n",
        "\n",
        "    # Compute the reduced gradient for y\n",
        "    return old_objective_function_gradient_y(reduced_Abar, reduced_x, reduced_y, beta), non_zero_indices_gradient_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg81Z8tX-iVK"
      },
      "source": [
        "# Frank Wolfe variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC3RuxV5jZ6_"
      },
      "source": [
        "**Common parts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pf29kM9_aHKR"
      },
      "outputs": [],
      "source": [
        "def plot_iteration_progress(iteration, verbosity, subplot, A, vertices_positions, x, y, number_of_steps=None):\n",
        "    \"\"\"\n",
        "    Plot the evolution of the x and y variables for a given iteration.\n",
        "\n",
        "    Parameters:\n",
        "    iteration (int): The current iteration number.\n",
        "    verbosity (int): Verbosity of the function, 0 is none, 1 is only at the end, 2 is for each iteration and 3 plots a graph (default is 0).\n",
        "    subplot: The subplot to update, used if verbosity is set to 3.\n",
        "    A (torch.Tensor): The adjacency tensor.\n",
        "    vertices_positions (torch.Tensor): The positions of vertices on a plane.\n",
        "    x (torch.Tensor): The x variable coding for the chosen vertices.\n",
        "    y (torch.Tensor): The y variable coding for the fake edges.\n",
        "    \"\"\"\n",
        "    if verbosity == 3:\n",
        "        subplot[1].clear()\n",
        "        plot_fake_edges_and_final_clique(A, vertices_positions, x, y, subplot)\n",
        "        subplot[1].set_title(\"Iteration %i\" % iteration)\n",
        "        plt.show()\n",
        "\n",
        "    if verbosity == 2:\n",
        "        if number_of_steps is None:\n",
        "            print(\"Iteration %i finished.\" % iteration)\n",
        "        else:\n",
        "            print(\"Iteration %i finished with %i small steps.\" % (iteration, number_of_steps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lmiER1bjkNm"
      },
      "source": [
        "**Parts of the FWdc**\n",
        "\n",
        "**Initialization:**\n",
        "- Initialize: $z_0 = (x_0, y_0) \\in \\Delta \\times \\mathcal{D}_s(G)$, $k = 0$\n",
        "\n",
        "**Algorithm Steps:**\n",
        "1. While $w_k$ is not stationary:\n",
        "  \n",
        "  a. Compute $x_{k+1}$ by applying one step of the previous algorithm with $w_0 = x_k$ and $f(w) = \\tilde{h}(w, y_k)$.\n",
        "  \n",
        "  b. Let $y_{k+1} \\in \\text{arg max}_{y\\in\\mathcal{D}_s(G)} \\nabla_y h_G(x_k,y_k)^Ty$.\n",
        "  \n",
        "  c. Set $k = k+1$.\n",
        "\n",
        "2. STOP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "P1oN00ajJlLo"
      },
      "outputs": [],
      "source": [
        "def FDFW_only_x_direction_stepsize(x, gradient):\n",
        "    \"\"\"\n",
        "    Compute the update direction and the maximum feasible step size for a step of the FDFW on x.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): The current point.\n",
        "    gradient (torch.Tensor): The gradient of the objective function at x.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the update direction (torch.Tensor) and the maximum feasible step size (float).\n",
        "    \"\"\"\n",
        "    # Find the indices of the components with the largest and smallest (only on the support of x) gradients\n",
        "    max_grad_index = torch.argmax(gradient)\n",
        "\n",
        "    nonzero_indices = torch.nonzero(x != 0, as_tuple=True)[0]\n",
        "    min_grad_index = nonzero_indices[torch.argmin(gradient[nonzero_indices])]\n",
        "\n",
        "    # Define the linear approximation direction\n",
        "    direction = torch.zeros_like(x)\n",
        "\n",
        "    dot_grad_wk = torch.dot(gradient, x)  # Corresponds to the dot product between the gradient and w_k in the algorithm\n",
        "\n",
        "    dot_grad_dfw = gradient[max_grad_index] - dot_grad_wk\n",
        "    dot_grad_dfd = dot_grad_wk - gradient[min_grad_index]\n",
        "\n",
        "    if dot_grad_dfw > dot_grad_dfd:\n",
        "        # d_FW direction\n",
        "        direction[max_grad_index] = 1\n",
        "        direction -= x\n",
        "        step_size_max = 1\n",
        "    else:\n",
        "        # d_FD direction\n",
        "        direction[min_grad_index] = -1\n",
        "        direction += x\n",
        "        step_size_max = torch.abs(x[min_grad_index] / direction[min_grad_index]).item() if direction[min_grad_index] != 0 else float('inf')  # To ensure that x stays above 0\n",
        "        # There is no need to ensure that x stays below 1 as it is always the case\n",
        "\n",
        "    return direction, step_size_max\n",
        "\n",
        "def FWdc_update_y(reduced_gradient, map_indices, y_shape, s, k_factorial):\n",
        "    \"\"\"\n",
        "    Update y for a step of the FWdc algorithm.\n",
        "\n",
        "    Parameters:\n",
        "    reduced_gradient (torch.Tensor): The gradient of the objective function with respect to y, in its reduced version to keep only the relevant values.\n",
        "    map_indices (list): List mapping the indices of the reduced gradient to the corresponding indices in the full gradient.\n",
        "    y_shape (tuple): Shape of y.\n",
        "    s (int): The number of fake edges to add.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "\n",
        "    Returns:\n",
        "    torch.Tensor: The updated y.\n",
        "    \"\"\"\n",
        "    # Get the indices of the s maximum values of the gradient\n",
        "    reduced_greatest_indices = s_max_indices(reduced_gradient, s, k_factorial)\n",
        "    greatest_indices = [\n",
        "        [map_indices[reduced_idx] for reduced_idx in array_dim] for array_dim in reduced_greatest_indices\n",
        "    ]\n",
        "\n",
        "    # Define the new y\n",
        "    y_new = torch.zeros(y_shape)\n",
        "\n",
        "    # Get all permutations of indices\n",
        "    k = len(y_shape)\n",
        "\n",
        "    if len(greatest_indices) > 0:\n",
        "        greatest_indices_with_permutations = np.empty(shape=(k, 0), dtype=int)\n",
        "        for perm in permutations(range(k)):\n",
        "            greatest_indices_with_permutations = [np.concatenate(\n",
        "                (greatest_indices_with_permutations[i], greatest_indices[perm[i]])) for i in range(k)]\n",
        "\n",
        "        # Assign the fake edges\n",
        "        y_new[greatest_indices_with_permutations] = 1\n",
        "\n",
        "    return y_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tHdnqWTlbGB9"
      },
      "outputs": [],
      "source": [
        "def FDFW_only_x(obj_func, grad_func, A, x0, n, s, k_factorial, max_iter=100, tol=1e-3, verbosity=0,\n",
        "                vertices_positions=None):\n",
        "    \"\"\"\n",
        "    Perform the FDFW on the x component.\n",
        "\n",
        "    Parameters:\n",
        "    obj_func (function): The objective function to minimize.\n",
        "    grad_func (function): The gradient function of the objective function with respect to x.\n",
        "    A (torch.Tensor): The adjacency tensor of shape (n, n, ..., n).\n",
        "    x0 (torch.Tensor): The starting point.\n",
        "    n (int): The dimensionality of the problem.\n",
        "    s (int): The maximum number of missing edges in the clique.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "    max_iter (int): The maximum number of iterations (default is 100).\n",
        "    tol (float): The tolerance for convergence (default is 1e-3).\n",
        "    verbosity (int): Verbosity of the function, 0 is none, 1 is only at the end, 2 is for each iteration and 3 plots a graph (default is 0).\n",
        "    vertices_positions (torch.Tensor): The positions of vertices on a plane, size (2, n), necessary if verbosity is set to 3.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the optimized x and the runtime of the function.\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    start_time = time.time()\n",
        "\n",
        "    x = x0\n",
        "\n",
        "    if verbosity == 3 and max_iter > 1:\n",
        "        subplot = plt.subplots()\n",
        "    else:\n",
        "        subplot = None\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < max_iter:\n",
        "        iteration += 1\n",
        "\n",
        "        # Direction for x\n",
        "        gradient = grad_func(x)\n",
        "        direction, step_size_max = FDFW_only_x_direction_stepsize(obj_func, grad_func, x, gradient)\n",
        "\n",
        "        # Perform Armijo line search to find the step size\n",
        "        step_size = step_size_max\n",
        "        while step_size > 1e-14 and obj_func(x + step_size * direction) < obj_func(x) + 0.1 * step_size * torch.dot(gradient, direction):\n",
        "            step_size *= 0.3\n",
        "\n",
        "        # Update the current point\n",
        "        x_new = x + step_size * direction\n",
        "\n",
        "        # Some tricks to address numerical approximation errors\n",
        "        x_new[x_new < 1e-13] = 0\n",
        "        x_new /= torch.sum(x_new)\n",
        "\n",
        "        # Check for convergence\n",
        "        if max_iter > 1:\n",
        "            FW_gap = torch.dot(gradient, x - x_new)\n",
        "\n",
        "            x = x_new\n",
        "\n",
        "            if is_s_defective_clique(A, x, s) and abs(FW_gap) < tol:\n",
        "                break\n",
        "\n",
        "            # Plot the progress\n",
        "            plot_iteration_progress(iteration, verbosity, subplot, A, vertices_positions, x, torch.zeros_like(A))\n",
        "        else:\n",
        "            x = x_new\n",
        "\n",
        "    # If used as a standalone function\n",
        "    if max_iter > 1:\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        if verbosity == 1 or verbosity == 2:\n",
        "            print(\"FDFW on x finished in %f s with %i iterations.\"%(elapsed_time, iteration))\n",
        "\n",
        "        return x, elapsed_time\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "def FWdc(obj_func, grad_func_x, reduced_grad_func_y, A, Abar, x0, y0, n, s, k_factorial, max_iter=100, tol=1e-3, verbosity=0,\n",
        "         vertices_positions=None):\n",
        "    \"\"\"\n",
        "    Perform the FWdc.\n",
        "\n",
        "    Parameters:\n",
        "    obj_func (function): The objective function to minimize.\n",
        "    grad_func_x (function): The gradient function of the objective function with respect to x.\n",
        "    reduced_grad_func_y (function): The gradient function of the objective function with respect to y, adjusted to compute only the relevant gradient values.\n",
        "    A (torch.Tensor): The adjacency tensor of shape (n, n, ..., n).\n",
        "    Abar (torch.Tensor): The complementary adjacency tensor of the same shape as A.\n",
        "    x0 (torch.Tensor): The starting point for x.\n",
        "    y0 (torch.Tensor): The starting point for y.\n",
        "    n (int): The dimensionality of the problem.\n",
        "    s (int): The maximum number of missing edges in the clique.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "    max_iter (int): The maximum number of iterations (default is 100).\n",
        "    tol (float): The tolerance for convergence (default is 1e-3).\n",
        "    verbosity (int): Verbosity of the function, 0 is none, 1 is only at the end, 2 is for each iteration and 3 plots a graph (default is 0).\n",
        "    vertices_positions (torch.Tensor): The positions of vertices on a plane, size (2, n), necessary if verbosity is set to 3.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the optimized x and y and the runtime of the function.\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    start_time = time.time()\n",
        "\n",
        "    x = x0\n",
        "    y = y0\n",
        "\n",
        "    if verbosity == 3:\n",
        "        subplot = plt.subplots()\n",
        "    else:\n",
        "        subplot = None\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < max_iter:\n",
        "        iteration += 1\n",
        "\n",
        "        # Update x\n",
        "        x_new = FDFW_only_x(lambda t: obj_func(t, y), lambda t: grad_func_x(t, y), A, x, n, s, k_factorial, max_iter=1, tol=1e-3, verbosity=0, vertices_positions=None)\n",
        "\n",
        "        # Update y\n",
        "        reduced_gradient_y, map_indices = reduced_grad_func_y(x_new, y)\n",
        "        y_new = FWdc_update_y(reduced_gradient_y, map_indices, y.shape, s, k_factorial)\n",
        "\n",
        "        # Check for convergence : support of x is an s-defective clique, FW gap < tol\n",
        "        FW_gap = torch.dot(grad_func_x(x, y), x - x_new)\n",
        "\n",
        "        x = x_new\n",
        "        y = y_new\n",
        "\n",
        "        if is_s_defective_clique(A, x, s) and abs(FW_gap) < tol:\n",
        "            break\n",
        "\n",
        "        # Plot the progress\n",
        "        plot_iteration_progress(iteration, verbosity, subplot, A, vertices_positions, x, y)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    if verbosity == 1 or verbosity == 2:\n",
        "        print(\"FWdc finished in %f s with %i iterations.\"%(elapsed_time, iteration))\n",
        "\n",
        "    return x, y, elapsed_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEyq1eYbjeH9"
      },
      "source": [
        "**Parts of the FDFW**\n",
        "\n",
        "The algorithm is the following :\n",
        "\n",
        "**Initialization:**  \n",
        "- Initialize: $w_0 \\in \\mathcal{Q}$, $k = 0$\n",
        "\n",
        "**Algorithm Steps:**  \n",
        "1. While $w_k$ is not stationary:\n",
        "   \n",
        "   a. Let $s_k \\in \\text{arg max}_{y\\in\\mathcal{Q}} \\nabla f(w_k)^Ty$ and $d_k^{\\mathcal{F}\\mathcal{W}} = s_k - w_k$.\n",
        "   \n",
        "   b. Let $v_k \\in \\text{arg min}_{y\\in\\mathcal{F}(w_k)} \\nabla f(w_k)^Ty$ and $d_k^{\\mathcal{F}\\mathcal{D}} = w_k - v_k$.\n",
        "   \n",
        "   c. If $\\nabla f(w_k)^Td_k^{\\mathcal{F}\\mathcal{W}} \\geq \\nabla f(w_k)^Td_k^{\\mathcal{F}\\mathcal{D}}$:\n",
        "      - $d_k = d_k^{\\mathcal{F}\\mathcal{W}}$\n",
        "   \n",
        "   d. Else:\n",
        "      - $d_k = d_k^{\\mathcal{F}\\mathcal{D}}$\n",
        "   \n",
        "   e. Choose the stepsize $\\alpha_k \\in (0, \\alpha_k^{\\max}]$ with a suitable criterion.\n",
        "   \n",
        "   f. Update $w_{k+1} = w_k + \\alpha_k d_k$.\n",
        "   \n",
        "   g. Set $k = k+1$.\n",
        "\n",
        "2. STOP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4BJvRpOpe85s"
      },
      "outputs": [],
      "source": [
        "def FDFW_direction_stepsize(x, y, gradient_x, reduced_gradient_y, map_indices, k_factorial):\n",
        "    \"\"\"\n",
        "    Compute the update direction and the maximum feasible step size for a step of the FDFW.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): The current point for x.\n",
        "    y (torch.Tensor): The current point for y.\n",
        "    gradient_x (torch.Tensor): The gradient of the objective function with respect to x at the current point.\n",
        "    reduced_gradient_y (torch.Tensor): The gradient of the objective function with respect to y at the current point, in its reduced version to keep only the relevant values.\n",
        "    map_indices (list): List mapping the indices of the reduced gradient to the corresponding indices in the full gradient.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the update direction for x (array), the update direction for y (array), and the maximum feasible step size (float).\n",
        "    \"\"\"\n",
        "    # Direction for x\n",
        "    # Find the indices of the components with the largest and smallest (only on the support of x) gradients\n",
        "    max_grad_index_x = torch.argmax(gradient_x)\n",
        "\n",
        "    nonzero_indices = torch.nonzero(x != 0, as_tuple=True)[0]\n",
        "    min_grad_index_x = nonzero_indices[torch.argmin(gradient_x[nonzero_indices])]\n",
        "\n",
        "    # Direction for y\n",
        "    reduced_indices_y = np.ix_(*[map_indices] * k)\n",
        "    y_reduced = y[reduced_indices_y]\n",
        "\n",
        "    reduced_greatest_indices = s_max_indices(reduced_gradient_y, s, k_factorial)\n",
        "    greatest_indices = [\n",
        "        [map_indices[reduced_idx] for reduced_idx in array_dim] for array_dim in reduced_greatest_indices\n",
        "    ]\n",
        "\n",
        "    y_equal_to_one_indices = torch.nonzero(y == 1, as_tuple=True)\n",
        "    reduced_y_equal_to_one_indices = torch.nonzero(y_reduced == 1, as_tuple=True)\n",
        "\n",
        "    support = y.clone()\n",
        "    support[y_equal_to_one_indices] = 0  # To discard these values\n",
        "\n",
        "    reduced_smallest_indices_in_support = s_minimal_indices_in_support(reduced_gradient_y, support[reduced_indices_y], s - len(y_equal_to_one_indices)//k_factorial, k_factorial)\n",
        "    smallest_indices_in_support = [\n",
        "        [map_indices[reduced_idx] for reduced_idx in array_dim] for array_dim in reduced_smallest_indices_in_support\n",
        "    ]\n",
        "\n",
        "    # Define the linear approximation direction for x and y\n",
        "    direction_x = torch.zeros_like(x)\n",
        "    direction_y = torch.zeros_like(y)\n",
        "\n",
        "    dot_grad_wk = torch.dot(gradient_x, x) + torch.nansum(torch.multiply(reduced_gradient_y, y_reduced))/k_factorial  # Corresponds to the dot product between the gradient and w_k in the algorithm\n",
        "\n",
        "    # greatest_indices and smallest_indices_in_support only contain s indices so no need to divide by k_factorial because there is no duplication\n",
        "    dot_grad_dfw = gradient_x[max_grad_index_x] + torch.sum(reduced_gradient_y[reduced_greatest_indices]) - dot_grad_wk\n",
        "    dot_grad_dfd = dot_grad_wk - gradient_x[min_grad_index_x] - torch.sum(reduced_gradient_y[reduced_smallest_indices_in_support]) - torch.sum(reduced_gradient_y[reduced_y_equal_to_one_indices])/k_factorial\n",
        "\n",
        "    if dot_grad_dfw > dot_grad_dfd:\n",
        "        # d_FW direction for x\n",
        "        direction_x[max_grad_index_x] = 1\n",
        "\n",
        "        # Get all permutations of indices for y\n",
        "        if len(greatest_indices) > 0:\n",
        "            greatest_indices_with_permutations = np.empty(shape=(k, 0), dtype=int)\n",
        "            for perm in permutations(range(k)):\n",
        "                greatest_indices_with_permutations = [np.concatenate((greatest_indices_with_permutations[i], greatest_indices[perm[i]])) for i in range(k)]\n",
        "\n",
        "            # Assign the fake edges for direction_y\n",
        "            direction_y[tuple(greatest_indices_with_permutations)] = 1\n",
        "\n",
        "        direction_x -= x\n",
        "        direction_y -= y\n",
        "\n",
        "        step_size_max = 1\n",
        "    else:\n",
        "        # d_FD direction for x\n",
        "        direction_x[min_grad_index_x] = -1\n",
        "\n",
        "        # Set direction for y equal to -1 where y equals one\n",
        "        direction_y[y_equal_to_one_indices] = -1\n",
        "\n",
        "        # Get all permutations of indices for y\n",
        "        if len(smallest_indices_in_support) > 0:\n",
        "            smallest_indices_with_permutations = np.empty(shape=(k, 0), dtype=int)\n",
        "            for perm in permutations(range(k)):\n",
        "                smallest_indices_with_permutations = [np.concatenate((smallest_indices_with_permutations[i], smallest_indices_in_support[perm[i]])) for i in range(k)]\n",
        "\n",
        "            # Assign the fake edges for direction_y\n",
        "            direction_y[tuple(smallest_indices_with_permutations)] = -1\n",
        "\n",
        "        direction_x += x\n",
        "        direction_y += y\n",
        "\n",
        "        step_size_max_x_array = torch.tensor([torch.abs(x[min_grad_index_x] / direction_x[min_grad_index_x]) if direction_x[min_grad_index_x] != 0 else float('inf')]) # To ensure that x stays above 0\n",
        "\n",
        "        step_size_max_y_array = torch.abs(y[direction_y < 0] / direction_y[direction_y < 0]).reshape(-1)  # To ensure that y stays above 0\n",
        "                                                        # There is no need to ensure that x and y stay below 1 as it is always the case\n",
        "\n",
        "        if torch.sum(direction_y) > 1e-13:  # Not exactly zero due to numerical imprecisions\n",
        "            step_size_max_y_array = torch.cat((step_size_max_y_array, torch.tensor([(s*k_factorial - torch.sum(y)) / torch.sum(direction_y)])))  # To ensure that 1^T y stays below s\n",
        "\n",
        "        step_size_max = torch.min(torch.cat((step_size_max_x_array, step_size_max_y_array))).item()\n",
        "\n",
        "    return direction_x, direction_y, step_size_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "A-ataXvsVG2-"
      },
      "outputs": [],
      "source": [
        "def FDFW(obj_func, grad_func_x, reduced_grad_func_y, A, Abar, x0, y0, n, s, k_factorial, max_iter=100, tol=1e-3, verbosity=0, vertices_positions=None):\n",
        "    \"\"\"\n",
        "    Perform the FDFW on both components.\n",
        "\n",
        "    Parameters:\n",
        "    obj_func (function): The objective function to minimize.\n",
        "    grad_func_x (function): The gradient function of the objective function with respect to x.\n",
        "    reduced_grad_func_y (function): The gradient function of the objective function with respect to y, adjusted to compute only the relevant gradient values.\n",
        "    A (torch.Tensor): The adjacency tensor of shape (n, n, ..., n).\n",
        "    Abar (torch.Tensor): The complementary adjacency tensor of the same shape as A.\n",
        "    x0 (torch.Tensor): The starting point for x.\n",
        "    y0 (torch.Tensor): The starting point for y.\n",
        "    n (int): The dimensionality of the problem.\n",
        "    s (int): The maximum number of missing edges in the clique.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "    max_iter (int): The maximum number of iterations (default is 100).\n",
        "    tol (float): The tolerance for convergence (default is 1e-3).\n",
        "    verbosity (int): Verbosity of the function, 0 is none, 1 is only at the end, 2 is for each iteration and 3 plots a graph (default is 0).\n",
        "    vertices_positions (torch.Tensor): The positions of vertices on a plane, size (2, n), necessary if verbosity is set to 3.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the optimized x and y and the runtime of the function.\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    start_time = time.time()\n",
        "\n",
        "    x = x0.clone()\n",
        "    y = y0.clone()\n",
        "\n",
        "    if verbosity == 3:\n",
        "        subplot = plt.subplots()\n",
        "    else:\n",
        "        subplot = None\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < max_iter:\n",
        "        iteration += 1\n",
        "\n",
        "        # Direction for x and y\n",
        "        gradient_x = grad_func_x(x, y)\n",
        "        reduced_gradient_y, map_indices = reduced_grad_func_y(x, y)\n",
        "\n",
        "        direction_x, direction_y, step_size_max = FDFW_direction_stepsize(obj_func, grad_func_x, reduced_grad_func_y, x, y, gradient_x, reduced_gradient_y, map_indices, s, k_factorial)\n",
        "\n",
        "        # Perform Armijo line search to find the step size\n",
        "        step_size = step_size_max\n",
        "        while step_size > 1e-14:\n",
        "            new_x = x + step_size * direction_x\n",
        "            new_y = y + step_size * direction_y\n",
        "            loss_new = obj_func(new_x, new_y)\n",
        "            loss = obj_func(x, y)\n",
        "            grad_dot_dir = torch.sum(torch.multiply(gradient_x, direction_x)) + torch.sum(torch.multiply(reduced_gradient_y, direction_y[map_indices]))\n",
        "            if loss_new < loss + 0.1 * step_size * grad_dot_dir:\n",
        "                break\n",
        "            step_size *= 0.3\n",
        "\n",
        "        # Update the current point\n",
        "        x.data.add_(step_size * direction_x)\n",
        "        y.data.add_(step_size * direction_y)\n",
        "\n",
        "        # Some tricks to address numerical approximation errors\n",
        "        x.data[x.data < 1e-13] = 0\n",
        "        y.data[y.data < 1e-13] = 0\n",
        "        y.data[y.data > 0.99] = 1\n",
        "\n",
        "        x.data /= torch.sum(x.data)\n",
        "\n",
        "        sum_y = torch.sum(y.data)\n",
        "        if sum_y > s * k_factorial:\n",
        "            y.data *= s * k_factorial / sum_y\n",
        "\n",
        "        # Check for convergence: support of x is an s-defective clique, FW gap < tol\n",
        "        FW_gap = torch.dot(grad_func_x(x, y), x - x.grad) + torch.nansum(reduced_gradient_y*(y - y.grad)[map_indices])/k_factorial\n",
        "\n",
        "        if is_s_defective_clique(A, x, s) and abs(FW_gap) < tol:\n",
        "            break\n",
        "\n",
        "        # Plot the progress\n",
        "        if verbosity == 2:\n",
        "            if iteration > 1:\n",
        "                print(\"Iteration %i finished.\" % (iteration - 1))\n",
        "        elif verbosity == 3:\n",
        "            plot_iteration_progress(iteration, verbosity, subplot, A, vertices_positions, x, y)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    if verbosity == 1 or verbosity == 2:\n",
        "        print(\"FDFW finished in %f s with %i iterations.\"%(elapsed_time, iteration))\n",
        "\n",
        "    return x, y, elapsed_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1mHAsno-rPv"
      },
      "source": [
        "# Short Step Chain implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdCkXxk5vdpu"
      },
      "source": [
        "Implementation of the Short Step Chain procedure detailed in [Avoiding bad steps in Frank Wolfe variants](https://arxiv.org/abs/2012.12737), which is a gradient recycling procedure aiming at improving existing rates of convergence for the FW algorithm and its variants. Considering a Frank Wolfe variant, the SSC subroutine replaces the part dedicated to the steps, and allows to skip gradients update when performing small steps :\n",
        "\n",
        "**Algorithm: FW container**\n",
        "\n",
        "**Initialization:**\n",
        "- Initialize: $x_0 \\in \\Omega$, $k = 0$\n",
        "\n",
        "**Algorithm Steps:**\n",
        "\n",
        "1. While $x_k$ is not stationary:\n",
        "   - $g = \\nabla f(x_k)$\n",
        "   - $x_{k+1} = SSC(x_k, g)$\n",
        "   - $k = k+1$\n",
        "\n",
        "where SSC is the following algorithm :\n",
        "\n",
        "**Algorithm: Short Step Chain Subroutine**\n",
        "\n",
        "**Initialization:**\n",
        "- Initialize: $y_0 = x$, $j = 0$\n",
        "\n",
        "**Algorithm Steps:**\n",
        "\n",
        "1. **Phase I:**\n",
        "   - Select $d_j \\in \\mathcal{A}(y_j, g)$, $\\alpha_{\\max}^{(j)} \\in \\alpha_{\\max}(y_j, d_j)$\n",
        "\n",
        "   - If $d_j = 0$, return $y_j$\n",
        "\n",
        "2. **Phase II:**\n",
        "   - Compute $\\beta_j$\n",
        "   - Let $\\alpha_j = \\min(\\alpha_{\\max}^{(j)}, \\beta_j)$\n",
        "   - $y_{j+1} = y_j + \\alpha_j d_j$\n",
        "\n",
        "   - If $\\alpha_j = \\beta_j$, return $y_{j+1}$\n",
        "\n",
        "3. $j = j+1$, repeat from the beginning of Phase I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlEoNQ3uOkT4"
      },
      "source": [
        "**Implementation of the SSC for the FWdc**\n",
        "\n",
        "Several parts of the naive implementation are reused, and the update for the $y$ component is put outside of the SSC procedure because the gradient is constant for its duration.\n",
        "\n",
        "The two solutions for the polynomial equations are linked to the two balls in the computation of $\\beta$, both of them having a real solution is equivalent to $y$ being in their intersection so no need to check it explicitely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "F-zVv3c1peYX"
      },
      "outputs": [],
      "source": [
        "def FDFW_only_x_short_step_chain_subroutine(x, gradient, L, max_iter=100):\n",
        "    \"\"\"\n",
        "    Perform the Short Step Chain phase of the FDFW on the x component.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): The current point.\n",
        "    gradient (torch.Tensor): The gradient of the objective function at x.\n",
        "    L (float): The approximation of the Lipschitz constant.\n",
        "    max_iter (int): The maximum number of iterations for Short Step Chain (default is 100).\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the updated point after the Short Step Chain phase and the number of small steps performed.\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    y = x\n",
        "    i = 0\n",
        "\n",
        "    while i < max_iter:\n",
        "        i += 1\n",
        "\n",
        "        # Phase I\n",
        "        direction, alpha_max = FDFW_only_x_direction_stepsize(y, gradient)\n",
        "\n",
        "        if torch.max(direction) == 0:\n",
        "            return y, i\n",
        "\n",
        "        # Phase II\n",
        "        # First ball\n",
        "        a1 = torch.sum(direction**2)\n",
        "        b1 = 2 * torch.dot(direction, y - x - gradient / (2 * L))\n",
        "        c1 = torch.sum((y - x - gradient / (2 * L))**2) - torch.sum(gradient**2) / (4 * L**2)\n",
        "\n",
        "        # Second ball\n",
        "        a2 = torch.sum(direction**2)\n",
        "        b2 = 2 * torch.dot(direction, y - x)\n",
        "        c2 = torch.sum((y - x)**2) - (torch.dot(gradient, direction) / L)**2\n",
        "\n",
        "        # Compute beta\n",
        "        if (b1**2 - 4*a1*c1 >= 0 and b2**2 - 4*a2*c2 >= 0): # If there are real solutions\n",
        "            solution_1 = (-b1 + torch.sqrt(b1**2 - 4 * a1 * c1)) / (2 * a1)\n",
        "            solution_2 = (-b2 + torch.sqrt(b2**2 - 4 * a2 * c2)) / (2 * a2)\n",
        "            beta = min(solution_1, solution_2)\n",
        "        else:\n",
        "            beta = 0\n",
        "\n",
        "        alpha = min(beta, alpha_max)\n",
        "        y = y + alpha * direction\n",
        "\n",
        "        if alpha == beta:\n",
        "            return y, i\n",
        "\n",
        "    # If the Short Step Chain chain did not converge within the maximum number of iterations\n",
        "    return y, i\n",
        "\n",
        "def FDFW_only_x_SSC(obj_func, grad_func, A, x0, L, n, s, k_factorial, max_iter=100, tol=1e-3, verbosity=0, vertices_positions=None):\n",
        "    \"\"\"\n",
        "    Perform the FDFW on the x component, with the Short Step Chain procedure.\n",
        "\n",
        "    Parameters:\n",
        "    obj_func (function): The objective function to minimize.\n",
        "    grad_func (function): The gradient function of the objective function with respect to x.\n",
        "    A (torch.Tensor): The adjacency tensor of shape (n, n, ..., n).\n",
        "    x0 (torch.Tensor): The starting point.\n",
        "    L (float): The approximation of the Lipschitz constant.\n",
        "    n (int): The dimensionality of the problem.\n",
        "    s (int): The number of top indices to consider.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "    max_iter (int): The maximum number of iterations (default is 100).\n",
        "    tol (float): The tolerance for convergence (default is 1e-3).\n",
        "    verbosity (int): Verbosity of the function, 0 is none, 1 is only at the end, 2 is for each iteration and 3 plots a graph (default is 0).\n",
        "    vertices_positions (torch.Tensor): The positions of vertices on a plane, size (2, n), necessary if verbosity is set to 3.\n",
        "\n",
        "    Returns:\n",
        "    tuple: An array containing the optimized x, and if max_iter is set to 1 the number of small steps performed, otherwise the runtime of the function.\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    start_time = time.time()\n",
        "\n",
        "    x = x0\n",
        "\n",
        "    if verbosity == 3:\n",
        "        subplot = plt.subplots()\n",
        "    else:\n",
        "        subplot = None\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < max_iter:\n",
        "        iteration += 1\n",
        "\n",
        "        gradient = grad_func(x)\n",
        "        x_new, n_steps = FDFW_only_x_short_step_chain_subroutine(x, gradient, L, max_iter=100)\n",
        "\n",
        "        # Improve the Lipschitz constant approximation\n",
        "        while obj_func(x) - obj_func(x_new) > 0.5 * torch.dot(gradient, x - x_new):\n",
        "            L *= 2.\n",
        "            x_new, n_steps = FDFW_only_x_short_step_chain_subroutine(x, gradient, L, max_iter=100)\n",
        "\n",
        "        # Some tricks to address numerical approximation errors\n",
        "        x_new[x_new < 1e-13] = 0\n",
        "        x_new /= torch.sum(x_new)\n",
        "\n",
        "        # Check for convergence\n",
        "        if max_iter > 1:\n",
        "            FW_gap = torch.dot(gradient, x - x_new)\n",
        "\n",
        "            x = x_new\n",
        "\n",
        "            if is_s_defective_clique(A, x, s) and abs(FW_gap) < tol:\n",
        "                break\n",
        "\n",
        "            # Plot the progress\n",
        "            plot_iteration_progress(iteration, verbosity, subplot, A, vertices_positions, x, np.zeros(A.shape), number_of_steps=n_steps)\n",
        "        else:\n",
        "            x = x_new\n",
        "\n",
        "    # If used as a standalone function\n",
        "    if max_iter > 1:\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "\n",
        "        if verbosity == 1 or verbosity == 2:\n",
        "            print(\"FDFW on x finished in %f s with %i iterations.\"%(elapsed_time, iteration))\n",
        "\n",
        "        return x, elapsed_time\n",
        "    else:\n",
        "        return x, L, n_steps\n",
        "\n",
        "def FWdc_SSC(obj_func, grad_func_x, reduced_grad_func_y, A, Abar, x0, y0, L0, n, s, k_factorial, stop_updating_y_after=None, max_iter=100, tol=1e-3, verbosity=0, vertices_positions=None):\n",
        "    \"\"\"\n",
        "    Perform the FWdc with the Short Step Chain procedure.\n",
        "\n",
        "    Parameters:\n",
        "    obj_func (function): The objective function to minimize.\n",
        "    grad_func_x (function): The gradient function of the objective function with respect to x.\n",
        "    reduced_grad_func_y (function): The gradient function of the objective function with respect to y, adjusted to compute only the relevant gradient values.\n",
        "    A (torch.Tensor): The adjacency tensor of shape (n, n, ..., n).\n",
        "    Abar (torch.Tensor): The complementary adjacency tensor of the same shape as A.\n",
        "    x0 (torch.Tensor): The starting point for x.\n",
        "    y0 (torch.Tensor): The starting point for y.\n",
        "    L0 (float): The approximation of the Lipschitz constant.\n",
        "    n (int): The dimensionality of the problem.\n",
        "    s (int): The maximum number of missing edges in the clique.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "    stop_updating_y_after (int): If set to an integer, when y doesn't change for this number of iteration its update will be skipped (default is None).\n",
        "    max_iter (int): The maximum number of iterations (default is 100).\n",
        "    tol (float): The tolerance for convergence (default is 1e-3).\n",
        "    verbosity (int): Verbosity of the function, 0 is none, 1 is only at the end, 2 is for each iteration and 3 plots a graph (default is 0).\n",
        "    vertices_positions (torch.Tensor): The positions of vertices on a plane, size (2, n), necessary if verbosity is set to 3.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the optimized x and y and the runtime of the function.\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    start_time = time.time()\n",
        "\n",
        "    x = x0\n",
        "    y = y0\n",
        "    L = L0\n",
        "\n",
        "    if verbosity == 3:\n",
        "        subplot = plt.subplots()\n",
        "    else:\n",
        "        subplot = None\n",
        "\n",
        "    iteration = 0\n",
        "    y_stationarity_counter = 0\n",
        "\n",
        "    while iteration < max_iter:\n",
        "        iteration += 1\n",
        "\n",
        "        # Update x\n",
        "        x_new, L, n_steps = FDFW_only_x_SSC(lambda t: obj_func(t, y), lambda t: grad_func_x(t, y), A, x, L, n, s, k_factorial, max_iter=1, tol=1e-3, verbosity=0, vertices_positions=None)\n",
        "\n",
        "        # Update y\n",
        "        if stop_updating_y_after == None or y_stationarity_counter < stop_updating_y_after: # The update is costly and y necessarily becomes stationary after a certain iteration\n",
        "            reduced_gradient_y, map_indices = reduced_grad_func_y(x_new, y)\n",
        "            y_new = FWdc_update_y(reduced_gradient_y, map_indices, y.shape, s, k_factorial)\n",
        "\n",
        "        # Update the counter for the stationarity of y and check for convergence : support of x is an s-defective clique, FW gap < tol\n",
        "        FW_gap = torch.dot(grad_func_x(x, y), x - x_new)\n",
        "\n",
        "        if stop_updating_y_after == None or y_stationarity_counter < stop_updating_y_after:\n",
        "            if torch.all(torch.eq(y_new, y)):\n",
        "                y_stationarity_counter += 1\n",
        "            else:\n",
        "                y_stationarity_counter = 0\n",
        "\n",
        "            y = y_new\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "        if is_s_defective_clique(A, x, s) and abs(FW_gap) < tol:\n",
        "            break\n",
        "\n",
        "        # Plot the progress\n",
        "        plot_iteration_progress(iteration, verbosity, subplot, A, vertices_positions, x, y, number_of_steps=n_steps)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    if verbosity == 1 or verbosity == 2:\n",
        "        print(\"FWdc finished in %f s with %i iterations.\"%(elapsed_time, iteration))\n",
        "\n",
        "    return x, y, elapsed_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2fhfKN6OMMY"
      },
      "source": [
        "**Implementation of the SSC for the FDFW**\n",
        "\n",
        "Several parts of the naive implementation are reused."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1lD-WlZxcUI9"
      },
      "outputs": [],
      "source": [
        "def FDFW_short_step_chain_subroutine(x, y, gradient_x, reduced_gradient_y, map_indices, L, s, k_factorial, max_iter=100):\n",
        "    \"\"\"\n",
        "    Perform the Short Step Chain phase of the FDFW.\n",
        "\n",
        "    Parameters:\n",
        "    x (torch.Tensor): The current point.\n",
        "    y (torch.Tensor): The current point.\n",
        "    gradient_x (torch.Tensor): The gradient of the objective function with respect to x at the current point.\n",
        "    reduced_gradient_y (torch.Tensor): The gradient function of the objective function with respect to y, adjusted to compute only the relevant gradient values.\n",
        "    map_indices (list): List mapping the indices of the reduced gradient to the corresponding indices in the full gradient.\n",
        "    L (float): The approximation of the Lipschitz constant.\n",
        "    s (int): The maximum number of missing edges in the clique.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "    max_iter (int): The maximum number of iterations for the Short Step Chain (default is 100).\n",
        "\n",
        "    Returns:\n",
        "    tuple: Two tensors containing the updated points after the Short Step Chain phase and the number of small steps performed.\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    t_x = x\n",
        "    t_y = y\n",
        "\n",
        "    reduced_indices_y = np.ix_(*[map_indices] * k)\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    while i < max_iter:\n",
        "        i += 1\n",
        "\n",
        "        # Phase I\n",
        "        direction_x, direction_y, alpha_max = FDFW_direction_stepsize(x, y, gradient_x, reduced_gradient_y, map_indices, k_factorial)\n",
        "\n",
        "        if torch.max(direction_x) == 0 and torch.max(direction_y) == 0:\n",
        "            return t_x, t_y\n",
        "\n",
        "        # Phase II\n",
        "        # Preprocessing\n",
        "        reduced_t_y_minus_y = (t_y - y)[reduced_indices_y]\n",
        "        reduced_direction_y = direction_y[reduced_indices_y]\n",
        "\n",
        "        # First ball\n",
        "        a1 = torch.sum(direction_x ** 2) + torch.sum(direction_y ** 2) / k_factorial\n",
        "        b1 = 2 * (\n",
        "            torch.dot(direction_x, t_x - x - gradient_x / (2 * L))\n",
        "            + torch.nansum(reduced_direction_y * (reduced_t_y_minus_y - reduced_gradient_y / (2 * L)) / k_factorial)\n",
        "        )\n",
        "        c1 = (\n",
        "            torch.sum((t_x - x - gradient_x / (2 * L)) ** 2)\n",
        "            + torch.nansum((reduced_t_y_minus_y - reduced_gradient_y / (2 * L)) ** 2 / k_factorial)\n",
        "            - torch.sum(gradient_x ** 2) / (4 * L ** 2)\n",
        "            - torch.nansum(reduced_gradient_y ** 2 / k_factorial) / (4 * L ** 2)\n",
        "        )\n",
        "\n",
        "        # Second ball\n",
        "        a2 = torch.sum(direction_x ** 2) + torch.sum(direction_y ** 2 / k_factorial)\n",
        "        b2 = 2 * (\n",
        "            torch.dot(direction_x, t_x - x)\n",
        "            + torch.nansum(direction_y*(t_y - y)) / k_factorial\n",
        "        )\n",
        "        c2 = (\n",
        "            torch.sum((t_x - x) ** 2)\n",
        "            + torch.sum((t_y - y) ** 2 / k_factorial)\n",
        "            - (torch.dot(gradient_x, direction_x) / L) ** 2\n",
        "            - (torch.nansum(reduced_gradient_y * reduced_direction_y) / k_factorial / L) ** 2\n",
        "        )\n",
        "\n",
        "        # Compute beta\n",
        "        if (b1**2 - 4*a1*c1 >= 0 and b2**2 - 4*a2*c2 >= 0): # If there are real solutions\n",
        "            solution_1 = (-b1 + torch.sqrt(b1**2 - 4 * a1 * c1)) / (2 * a1)\n",
        "            solution_2 = (-b2 + torch.sqrt(b2**2 - 4 * a2 * c2)) / (2 * a2)\n",
        "            beta = min(solution_1, solution_2)\n",
        "        else:\n",
        "            beta = 0\n",
        "\n",
        "        alpha = min(beta, alpha_max)\n",
        "        t_x = t_x + alpha * direction_x\n",
        "        t_y = t_y + alpha * direction_y\n",
        "\n",
        "        if alpha == beta:\n",
        "            return t_x, t_y, i\n",
        "\n",
        "    # If the Short Step Chain did not converge within the maximum number of iterations\n",
        "    return t_x, t_y, i\n",
        "\n",
        "def FDFW_SSC(obj_func, grad_func_x, reduced_grad_func_y, A, Abar, x0, y0, L0, n, s, k_factorial, max_iter=100,\n",
        "              tol=1e-3, verbosity=0, vertices_positions=None):\n",
        "    \"\"\"\n",
        "    Perform the FDFW with the Short Step Chain procedure.\n",
        "\n",
        "    Parameters:\n",
        "    obj_func (function): The objective function to minimize.\n",
        "    grad_func_x (function): The gradient function of the objective function with respect to x.\n",
        "    reduced_grad_func_y (function): The gradient function of the objective function with respect to y, adjusted to compute only the relevant gradient values.\n",
        "    A (torch.Tensor): The adjacency tensor of shape (n, n, ..., n).\n",
        "    Abar (torch.Tensor): The complementary adjacency tensor of the same shape as A.\n",
        "    x0 (torch.Tensor): The starting point for x.\n",
        "    y0 (torch.Tensor): The starting point for y.\n",
        "    L0 (float): The approximation of the Lipschitz constant.\n",
        "    n (int): The dimensionality of the problem.\n",
        "    s (int): The maximum number of missing edges in the clique.\n",
        "    k_factorial (int): The preprocessed value of k!.\n",
        "    max_iter (int): The maximum number of iterations (default is 100).\n",
        "    tol (float): The tolerance for convergence (default is 1e-3).\n",
        "    verbosity (int): Verbosity of the function, 0 is none, 1 is only at the end, 2 is for each iteration and 3 plots a graph (default is 0).\n",
        "    vertices_positions (torch.Tensor): The positions of vertices on a plane, size (2, n), necessary if verbosity is set to 3.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the optimized x and y and the runtime of the function.\n",
        "    \"\"\"\n",
        "    # Initialization\n",
        "    start_time = time.time()\n",
        "\n",
        "    x = x0\n",
        "    y = y0\n",
        "    L = L0\n",
        "\n",
        "    if verbosity == 3:\n",
        "        subplot = plt.subplots()\n",
        "    else:\n",
        "        subplot = None\n",
        "\n",
        "    iteration = 0\n",
        "\n",
        "    while iteration < max_iter:\n",
        "        iteration += 1\n",
        "\n",
        "        gradient_x = grad_func_x(x, y)\n",
        "        reduced_gradient_y, map_indices = reduced_grad_func_y(x, y)\n",
        "        x_new, y_new, n_steps = FDFW_short_step_chain_subroutine(\n",
        "            x, y, gradient_x, reduced_gradient_y, map_indices, L, s, k_factorial, max_iter=100\n",
        "        )\n",
        "\n",
        "        # Improve the Lipschitz constant approximation\n",
        "        reduced_indices_y = np.ix_(*[map_indices] * k)\n",
        "\n",
        "        while (\n",
        "            obj_func(x, y) - obj_func(x_new, y_new)\n",
        "            > 0.5\n",
        "            * (\n",
        "                torch.dot(gradient_x, x - x_new)\n",
        "                + torch.nansum(reduced_gradient_y * (y - y_new)[reduced_indices_y])\n",
        "            )\n",
        "        ):\n",
        "            L *= 2.\n",
        "            x_new, y_new, n_steps = FDFW_short_step_chain_subroutine(\n",
        "                x, y, gradient_x, reduced_gradient_y, map_indices, L, s, k_factorial, max_iter=100\n",
        "            )\n",
        "\n",
        "        # Some tricks to address numerical approximation errors\n",
        "        x_new[x_new < 1e-13] = 0\n",
        "        y_new[y_new < 1e-13] = 0\n",
        "        y_new[y_new > 0.99] = 1\n",
        "\n",
        "        x_new /= torch.sum(x_new)\n",
        "\n",
        "        sum_y = torch.sum(y_new)\n",
        "        if sum_y > s * k_factorial:\n",
        "            y_new *= s * k_factorial / sum_y\n",
        "\n",
        "        # Check for convergence: support of x is an s-defective clique, FW gap < tol and y is an integer vector\n",
        "        FW_gap = (\n",
        "            torch.dot(grad_func_x(x, y), x - x_new)\n",
        "            + torch.nansum(reduced_gradient_y * (y - y_new)[reduced_indices_y]) / k_factorial\n",
        "        )\n",
        "\n",
        "        x = x_new\n",
        "        y = y_new\n",
        "\n",
        "        if is_s_defective_clique(A, x, s) and abs(FW_gap) < tol:\n",
        "            break\n",
        "\n",
        "        # Plot the progress\n",
        "        plot_iteration_progress(iteration, verbosity, subplot, A, vertices_positions, x, y, number_of_steps=n_steps)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    if verbosity == 1 or verbosity == 2:\n",
        "        print(\"FDFW finished in %f s with %i iterations.\"%(elapsed_time, iteration))\n",
        "\n",
        "    return x, y, elapsed_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00D9meekxIHT"
      },
      "source": [
        "# Results presentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2buRstKbf_hL"
      },
      "outputs": [],
      "source": [
        "def box_and_whisker_plot(data, axis_labels, data_labels=None, labels_rotation=0, y_scale='linear', title=None):\n",
        "    \"\"\"\n",
        "    Create a box and whisker plot for the given datasets. Each dictionary in data corresponds to a specific dataset where the keys are the values of the varying parameters.\n",
        "    For example, to plot the results of the FWdc and the FDFW with s varying, there will be two dictionaries in data, one for the FWdc and the other for the FDFW, and the keys in the dictionaries will be the varying values of s as strings.\n",
        "\n",
        "    Parameters:\n",
        "    data (tuple of dict of arrays): Tuple containing the dictionaries of data.\n",
        "    axis_labels (list of str): Labels for x and y axes.\n",
        "    data_labels (list of str): Labels for each of the dictionaries (default is None i.e. no labels).\n",
        "    labels_rotation(int): Rotation for the data labels.\n",
        "    y_scale (str): Scale of the y-axis (default is 'linear').\n",
        "    title (str): Title for the plot (default is None i.e. no title).\n",
        "    \"\"\"\n",
        "    n = len(data)  # Number of datasets\n",
        "    m = len(data[0].keys())  # Number of values of the varying parameter\n",
        "\n",
        "    # Create x-axis positions for data points\n",
        "    x = np.arange(m * (2 * n + 2))  # n as the number of columns plotted, one space between them\n",
        "                                    # and two spaces between each value of the varying parameter\n",
        "\n",
        "    # Create a new figure and axis\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Set y-axis scale\n",
        "    ax.set_yscale(y_scale)\n",
        "\n",
        "    # Plot individual box and whisker plots\n",
        "    boxprops = dict(linewidth=1, edgecolor='blue', facecolor='white')\n",
        "    whiskerprops = dict(linewidth=1, linestyle='dashed', dash_capstyle='butt', color='blue')\n",
        "    medianprops = dict(linewidth=1, color='red')\n",
        "\n",
        "    for i in range(n):\n",
        "        for j, d in enumerate(data[i]):\n",
        "            # Plot individual data points\n",
        "            ax.scatter(np.full(len(data[i][d]), x[i * 2 + j * (2 * n + 2)]), data[i][d], color='red', marker='+')\n",
        "\n",
        "            # Create the box-and-whisker plot\n",
        "            ax.boxplot(data[i][d], vert=True, positions=[x[i * 2 + j * (2 * n + 2)]], widths=0.8, patch_artist=True, showfliers=False, boxprops=boxprops, whiskerprops=whiskerprops, medianprops=medianprops)\n",
        "\n",
        "    # Display the number 'i' below each box and whisker plot, this is necessarily outside the above loop as we need the final size of the plot data area in order to convert the coordinates to absolute ones\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            x_absolute = (x[i * 2 + j * (2 * n + 2)] - ax.get_xlim()[0]) / (ax.get_xlim()[1] - ax.get_xlim()[0]) # The x value is relative to the plot data area and is not absolute\n",
        "            ax.text(x_absolute, -0.05, str(i), ha='center', transform=ax.transAxes)\n",
        "\n",
        "    # Set axis labels and title\n",
        "    ax.set_xlabel(axis_labels[0])\n",
        "    ax.set_ylabel(axis_labels[1])\n",
        "\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "\n",
        "    # Set x-axis ticks and labels\n",
        "    ax.set_xticks(x[(2*n-1) // 2::(2 * n + 2)])\n",
        "    ax.set_xticklabels([d for d in data[0]], y=-0.05, rotation=labels_rotation)\n",
        "\n",
        "    # Create a legend outside the graph\n",
        "    if data_labels:\n",
        "        legend_elements = []\n",
        "        for i, label in enumerate(data_labels):\n",
        "            legend_elements.append(plt.Line2D([0], [0], label=f'[{i}]: {label}'))\n",
        "\n",
        "        # Hide the lines so only the labels appear\n",
        "        ax.legend(handles=legend_elements, bbox_to_anchor=(1, 1), loc='upper left',\n",
        "        handlelength=0,  # Set the handle length to zero to hide lines or markers\n",
        "        handletextpad=0  # Set the handle text pad to zero to position text labels close to the legend\n",
        "        )\n",
        "\n",
        "    # Adjust layout and show the plot\n",
        "    plt.tight_layout()\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USMFoUbOFm72"
      },
      "source": [
        "# Demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDK2DMuPFvpt"
      },
      "source": [
        "This section is a demo of the algorithm. It generates a random hypergraph with the parameters provided and display the end result. It is also possible to display the evolution of the solution at each iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iYH1XlXtO5n"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "k = 3  # Number of vertices in an edge\n",
        "s = 4  # Maximum number of missing edges in the clique\n",
        "nd = 10  # Number of different values for x and y in the random hypergraph\n",
        "prob = 0.5  # Probability of an edge existing in the random hypergraph\n",
        "verbose = 1  # 0 for no verbosity, 1 for a message at the end of the run, 2 for each iteration, and 3 for a graph at each iteration\n",
        "\n",
        "L0 = 1  # Initial value for the approximation of the Lipschitz constant in the SSC\n",
        "stop_updating_y_after = None  # Skips the update of y after a certain number of iterations with no changes, set to None to perform all updates\n",
        "max_iter = np.inf  # Max number of iterations for each algorithm\n",
        "tol = np.inf  # Tolerance for convergence, if np.inf then the algorithm stops as soon as an s-defective clique is identified\n",
        "\n",
        "\n",
        "# Others parameters, leave untouched\n",
        "k_factorial = math.factorial(k)  # Preprocessing\n",
        "n = nd**2\n",
        "alpha = 0.45 if k == 2 else 1 / (k * (k - 1))\n",
        "beta = n**(-k) / 2\n",
        "\n",
        "x = np.linspace(0, 1, nd)\n",
        "y = np.linspace(0, 1, nd)\n",
        "xx, yy = np.meshgrid(x, y)\n",
        "\n",
        "vertices = np.vstack((xx.ravel(), yy.ravel()))\n",
        "\n",
        "# Generate a random hypergraph\n",
        "A = generate_random_adjacency_tensor(k, n, prob)\n",
        "Abar = complementary_hypergraph(A)\n",
        "\n",
        "# Define objective functions and gradients\n",
        "objective = lambda x, y: objective_function(Abar, x, y, k, k_factorial, alpha, beta)\n",
        "gradient_fx = lambda x, y: objective_function_gradient_x(Abar, x, y, k, k_factorial, alpha)\n",
        "reduced_gradient_fy = lambda x, y: objective_function_reduced_gradient_y(Abar, x, y, k_factorial, beta)\n",
        "\n",
        "objective_no_y = lambda x: objective_function(Abar, x, torch.zeros(A.shape), k, k_factorial, alpha, beta)\n",
        "gradient_fx_no_y = lambda x: objective_function_gradient_x(Abar, x, torch.zeros(A.shape), k, k_factorial, alpha)\n",
        "\n",
        "# Initialize x0 randomly on the simplex\n",
        "x0 = torch.tensor(np.random.dirichlet(np.ones(n)), dtype=torch.float64)\n",
        "\n",
        "# Initialize y0 as zeroes\n",
        "y0 = torch.zeros(A.shape)\n",
        "\n",
        "# Run the optimization\n",
        "x_opt1, y_opt1, runtime = FWdc_SSC(objective, gradient_fx, reduced_gradient_fy, A, Abar, x0, y0, L0, n, s, k_factorial, stop_updating_y_after, max_iter, tol, verbosity=verbose, vertices_positions=vertices)\n",
        "x_opt2, y_opt2, runtime = FDFW_SSC(objective, gradient_fx, reduced_gradient_fy, A, Abar, x0, y0, L0, n, s, k_factorial, max_iter, tol, verbosity=verbose, vertices_positions=vertices)\n",
        "x_opt3, runtime = FDFW_only_x_SSC(objective_no_y, gradient_fx_no_y, A, x0, L0, n, s, k_factorial, max_iter, tol, verbosity=verbose, vertices_positions=vertices)\n",
        "\n",
        "# Print the final sizes of the s-defective cliques and plot it\n",
        "# FWdc\n",
        "print(f\"\\n FWdc found a {s}-defective clique of size {torch.sum(x_opt1 > 0)}.\")\n",
        "\n",
        "if verbose != 3:\n",
        "    plot_fake_edges_and_final_clique(A, vertices, x_opt1, y_opt1)\n",
        "\n",
        "# FDFW\n",
        "print(f\"\\n FDFW found a {s}-defective clique of size {torch.sum(x_opt2 > 0)}.\")\n",
        "\n",
        "if verbose != 3:\n",
        "    # Extracting y because y_e = 0 or 1 is not enforced (we stop before convergence)\n",
        "    y_opt2_real = torch.zeros(A.shape)\n",
        "\n",
        "    for idx in list(combinations(torch.where(x_opt2 > 0)[0], k)):\n",
        "        y_opt2_real[idx] = 1\n",
        "\n",
        "    y_opt2_real[A == 1] = 0\n",
        "\n",
        "    # Plot\n",
        "    plot_fake_edges_and_final_clique(A, vertices, x_opt2, y_opt2_real)\n",
        "\n",
        "# FDFW on x\n",
        "print(f\"\\n FDFW on x found a {s}-defective clique of size {torch.sum(x_opt3 > 0)}.\")\n",
        "\n",
        "if verbose != 3:\n",
        "    # Extracting y\n",
        "    y_opt3 = torch.zeros(A.shape)\n",
        "\n",
        "    for idx in list(combinations(torch.where(x_opt3 > 0)[0], k)):\n",
        "        y_opt3[idx] = 1\n",
        "\n",
        "    y_opt3[A == 1] = 0\n",
        "\n",
        "    # Plot\n",
        "    plot_fake_edges_and_final_clique(A, vertices, x_opt3, y_opt3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHuZ_4nDdlWF"
      },
      "source": [
        "# Comparison between the previous and the novel formulations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVvV8qsBd-cp"
      },
      "source": [
        "This section compares the solution quality of the two formulations (for $k=2$), optimized with the FDFW and the FWdc algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmYuXmKZIB4Z"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "graph_names = [\n",
        "    \"C125.9\", \"C250.9\", \"brock200_2\", \"brock200_4\", \"keller4\", \"p_hat300-1\", \"p_hat300-2\", \"p_hat300-3\"\n",
        "]  # List of graph names on which the two formulations will be tested\n",
        "\n",
        "s = 4  # Maximum number of missing edges in the clique\n",
        "\n",
        "L0 = 1  # Initial value for the approximation of the Lipschitz constant in the SSC\n",
        "max_number_sampling = 100  # Maximum number of restarts for each algorithm on each graph\n",
        "max_time_allowed_in_seconds = 3600  # Maximum time allowed for the restarts, finishes the last execution and stops further restarts\n",
        "stop_updating_y_after = None  # Skips the update of y after a certain number of iterations with no changes, set to None to perform all updates\n",
        "max_iter = np.inf  # Max number of iterations for each algorithm\n",
        "tol = np.inf  # Tolerance for convergence, if np.inf then the algorithm stops as soon as an s-defective clique is identified\n",
        "\n",
        "# Preprocessing\n",
        "k = 2\n",
        "k_factorial = math.factorial(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSmxVLyRIbiU"
      },
      "outputs": [],
      "source": [
        "# Call the function to download and convert the graphs\n",
        "graph_data = download_and_convert_DIMACS_graphs(graph_names, k)\n",
        "\n",
        "# Initialize data storage\n",
        "support_sizes_FWdc_old = {}\n",
        "runtimes_FWdc_old = {}\n",
        "\n",
        "support_sizes_FWdc_new = {}\n",
        "runtimes_FWdc_new = {}\n",
        "\n",
        "support_sizes_FDFW_old = {}\n",
        "runtimes_FDFW_old = {}\n",
        "\n",
        "support_sizes_FDFW_new = {}\n",
        "runtimes_FDFW_new = {}\n",
        "\n",
        "# Access the k-graph tensor for each graph\n",
        "for graph_name, k_graph_tensor in graph_data.items():\n",
        "    # Initialize the time counters\n",
        "    time_counter_FWdc_old = 0\n",
        "    time_counter_FWdc_new = 0\n",
        "    time_counter_FDFW_old = 0\n",
        "    time_counter_FDFW_new = 0\n",
        "\n",
        "    # Initialize dictionaries to store results for each graph\n",
        "    support_sizes_FWdc_old[graph_name] = []\n",
        "    runtimes_FWdc_old[graph_name] = []\n",
        "\n",
        "    support_sizes_FWdc_new[graph_name] = []\n",
        "    runtimes_FWdc_new[graph_name] = []\n",
        "\n",
        "    support_sizes_FDFW_old[graph_name] = []\n",
        "    runtimes_FDFW_old[graph_name] = []\n",
        "\n",
        "    support_sizes_FDFW_new[graph_name] = []\n",
        "    runtimes_FDFW_new[graph_name] = []\n",
        "\n",
        "    print(f\"Currently doing graph: {graph_name}\")\n",
        "\n",
        "    # Extract graph information\n",
        "    A = k_graph_tensor\n",
        "    Abar = complementary_hypergraph(A)\n",
        "    n = A.shape[0]\n",
        "\n",
        "    # Define regularization parameters\n",
        "    # Old formulation\n",
        "    alpha_old = 1\n",
        "    beta_old = 2 / n**2\n",
        "\n",
        "    # New formulation\n",
        "    alpha_new = 0.45\n",
        "    beta_new = n**(-2) / 2\n",
        "\n",
        "    # Define objective functions and gradients\n",
        "    objective_old = lambda x, y: old_objective_function(A, x, y, alpha_old, beta_old)\n",
        "    gradient_fx_old = lambda x, y: old_objective_function_gradient_x(A, x, y, alpha_old)\n",
        "    reduced_gradient_fy_old = lambda x, y: old_objective_function_reduced_gradient_y(Abar, x, y, beta_old)\n",
        "\n",
        "    objective_new = lambda x, y: objective_function(Abar, x, y, k, k_factorial, alpha_new, beta_new)\n",
        "    gradient_fx_new = lambda x, y: objective_function_gradient_x(Abar, x, y, k, k_factorial, alpha_new)\n",
        "    reduced_gradient_fy_new = lambda x, y: objective_function_reduced_gradient_y(Abar, x, y, k_factorial, beta_new)\n",
        "\n",
        "    for i in range(max_number_sampling):\n",
        "        # Initialize x0 randomly on the simplex\n",
        "        x0 = torch.tensor(np.random.dirichlet(np.ones(n)), dtype=torch.float64)\n",
        "\n",
        "        # Initialize y0 as zeroes\n",
        "        y0 = torch.zeros(A.shape)\n",
        "\n",
        "        # Run the FWdc optimization with the old formulation\n",
        "        # Stop the restarts if the maximum allowed time has been hit\n",
        "        if time_counter_FWdc_old < max_time_allowed_in_seconds:\n",
        "            x_opt, y_opt, runtime = FWdc_SSC(objective_old, gradient_fx_old, reduced_gradient_fy_old, A, Abar, x0, y0, L0, n, s, k_factorial, stop_updating_y_after, max_iter, tol, verbosity=1)\n",
        "            time_counter_FWdc_old += runtime\n",
        "\n",
        "            if is_s_defective_clique(A, x_opt, s):\n",
        "                # Calculate the size of the support of x\n",
        "                support_size = torch.sum(x_opt > 0).item()\n",
        "\n",
        "                # Store the results\n",
        "                support_sizes_FWdc_old[graph_name].append(support_size)\n",
        "                runtimes_FWdc_old[graph_name].append(runtime)\n",
        "            else:\n",
        "                print(\"FWdc did not converge.\")\n",
        "\n",
        "        # Run the FWdc optimization with the new formulation\n",
        "        # Stop the restarts if the maximum allowed time has been hit\n",
        "        if time_counter_FWdc_new < max_time_allowed_in_seconds:\n",
        "            x_opt, y_opt, FWdc_SSC(objective_new, gradient_fx_new, reduced_gradient_fy_new, A, Abar, x0, y0, L0, n, s, k_factorial, stop_updating_y_after, max_iter, tol, verbosity=1)\n",
        "            time_counter_FWdc_new += runtime\n",
        "\n",
        "            if is_s_defective_clique(A, x_opt, s):\n",
        "                # Calculate the size of the support of x\n",
        "                support_size = torch.sum(x_opt > 0).item()\n",
        "\n",
        "                # Store the results\n",
        "                support_sizes_FWdc_new[graph_name].append(support_size)\n",
        "                runtimes_FWdc_new[graph_name].append(runtime)\n",
        "            else:\n",
        "                print(\"FWdc did not converge.\")\n",
        "\n",
        "        # Run the FDFW optimization with the old formulation\n",
        "        # Stop the restarts if the maximum allowed time has been hit\n",
        "        if time_counter_FDFW_old < max_time_allowed_in_seconds:\n",
        "            x_opt, y_opt, runtime = FDFW_SSC(objective_old, gradient_fx_old, reduced_gradient_fy_old, A, Abar, x0, y0, L0, n, s, k_factorial, max_iter, tol, verbosity=1)\n",
        "            time_counter_FDFW_old += runtime\n",
        "\n",
        "            if is_s_defective_clique(A, x_opt, s):\n",
        "                # Calculate the size of the support of x\n",
        "                support_size = torch.sum(x_opt > 0).item()\n",
        "\n",
        "                # Store the results\n",
        "                support_sizes_FDFW_old[graph_name].append(support_size)\n",
        "                runtimes_FDFW_old[graph_name].append(runtime)\n",
        "            else:\n",
        "                print(\"FDFW did not converge.\")\n",
        "\n",
        "        # Run the FDFW optimization with the new formulation\n",
        "        # Stop the restarts if the maximum allowed time has been hit\n",
        "        if time_counter_FDFW_new < max_time_allowed_in_seconds:\n",
        "            x_opt, y_opt, runtime = FDFW_SSC(objective_new, gradient_fx_new, reduced_gradient_fy_new, A, Abar, x0, y0, L0, n, s, k_factorial, max_iter, tol, verbosity=1)\n",
        "            time_counter_FDFW_new += runtime\n",
        "\n",
        "            if is_s_defective_clique(A, x_opt, s):\n",
        "                # Calculate the size of the support of x\n",
        "                support_size = torch.sum(x_opt > 0).item()\n",
        "\n",
        "                # Store the results\n",
        "                support_sizes_FDFW_new[graph_name].append(support_size)\n",
        "                runtimes_FDFW_new[graph_name].append(runtime)\n",
        "            else:\n",
        "                print(\"FDFW did not converge.\")\n",
        "\n",
        "# Create a new dictionary for the normalized clique sizes\n",
        "normalized_support_sizes_FWdc_old = {}\n",
        "normalized_support_sizes_FWdc_new = {}\n",
        "normalized_support_sizes_FDFW_old = {}\n",
        "normalized_support_sizes_FDFW_new = {}\n",
        "\n",
        "# Iterate through the original dictionary\n",
        "for key, values in support_sizes_FWdc_old.items():\n",
        "    normalized_support_sizes_FWdc_old[key] = [value/best_known_cliques_sizes[key] for value in support_sizes_FWdc_old[key]]\n",
        "    normalized_support_sizes_FWdc_new[key] = [value/best_known_cliques_sizes[key] for value in support_sizes_FWdc_new[key]]\n",
        "    normalized_support_sizes_FDFW_old[key] = [value/best_known_cliques_sizes[key] for value in support_sizes_FDFW_old[key]]\n",
        "    normalized_support_sizes_FDFW_new[key] = [value/best_known_cliques_sizes[key] for value in support_sizes_FDFW_new[key]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLeqbkG0pc0v"
      },
      "outputs": [],
      "source": [
        "# Plot the normalized sizes of the cliques found for each graph\n",
        "box_and_whisker_plot(\n",
        "    (normalized_support_sizes_FWdc_old, normalized_support_sizes_FWdc_new, normalized_support_sizes_FDFW_old, normalized_support_sizes_FDFW_new),\n",
        "    [\"Graph\", \"Ratio (Found s-defective clique size) \\n / (Best known clique size)\"],\n",
        "    [\"FWdc (old formulation)\", \"FWdc (new formulation)\", \"FDFW on x (old formulation)\", \"FDFW on x (new formulation)\"],\n",
        "    labels_rotation=270,\n",
        "    title=\"Normalized sizes of the s-defective \\n cliques found for each graph (k = %i, s = %i)\" % (k, s)\n",
        ")\n",
        "\n",
        "# Plot the runtimes for each graph (log scale)\n",
        "box_and_whisker_plot(\n",
        "    (runtimes_FWdc_old, runtimes_FWdc_new, runtimes_FDFW_old, runtimes_FDFW_new),\n",
        "    [\"Graph\", \"Runtime (in seconds)\"],\n",
        "    [\"FWdc (old formulation)\", \"FWdc (new formulation)\", \"FDFW on x (old formulation)\", \"FDFW on x (new formulation)\"],\n",
        "    labels_rotation=270,\n",
        "    y_scale='log',\n",
        "    title=\"Runtimes for each graph (k = %i, s = %i)\" % (k, s)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obf1MftwJBzb"
      },
      "source": [
        "# Application to the DIMACS dataset (varying graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAYKFMtB_uX-"
      },
      "source": [
        "This section compares the solution quality and the runtime between the FWdc, the full FDFW and a simple FDFW on x stopping as soon as the support of x is an $s$-defective clique. The following part compares the performances on each graph, with a single value for s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bhHGo-PJT-H"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "k = 3  # Number of vertices in an edge\n",
        "\n",
        "# Parameters\n",
        "graph_names = [\n",
        "    \"C125.9\", \"C250.9\", \"brock200_2\", \"brock200_4\",\n",
        "    \"keller4\", \"p_hat300-1\", \"p_hat300-2\", \"p_hat300-3\"\n",
        "]  # List of graph names on which the two formulations will be tested\n",
        "\n",
        "s = 4  # Maximum number of missing edges in the clique\n",
        "\n",
        "L0 = 1  # Initial value for the approximation of the Lipschitz constant in the SSC\n",
        "max_number_sampling = 100  # Maximum number of restarts for each algorithm on each graph\n",
        "max_time_allowed_in_seconds = 3600  # Maximum time allowed for the restarts, finishes the last execution and stops further restarts\n",
        "stop_updating_y_after = None  # Skips the update of y after a certain number of iterations with no changes, set to None to perform all updates\n",
        "max_iter = np.inf  # Max number of iterations for each algorithm\n",
        "tol = np.inf  # Tolerance for convergence, if np.inf then the algorithm stops as soon as an s-defective clique is identified\n",
        "\n",
        "# Preprocessing\n",
        "k_factorial = math.factorial(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra_I1h0pd9zn"
      },
      "outputs": [],
      "source": [
        "# Call the function to download and convert the graphs\n",
        "graph_data = download_and_convert_DIMACS_graphs(graph_names, k)\n",
        "\n",
        "# Initialize data storage\n",
        "support_sizes_FWdc = {}\n",
        "runtimes_FWdc = {}\n",
        "\n",
        "support_sizes_FDFW = {}\n",
        "runtimes_FDFW = {}\n",
        "\n",
        "support_sizes_FDFW_on_x = {}\n",
        "runtimes_FDFW_on_x = {}\n",
        "\n",
        "# Access the k-graph tensor for each graph\n",
        "for graph_name, k_graph_tensor in graph_data.items():\n",
        "    # Initialize the time counters\n",
        "    time_counter_FWdc = 0\n",
        "    time_counter_FDFW = 0\n",
        "    time_counter_FDFW_on_x = 0\n",
        "\n",
        "    # Initialize dictionaries to store results for each graph\n",
        "    support_sizes_FWdc[graph_name] = []\n",
        "    runtimes_FWdc[graph_name] = []\n",
        "\n",
        "    support_sizes_FDFW[graph_name] = []\n",
        "    runtimes_FDFW[graph_name] = []\n",
        "\n",
        "    support_sizes_FDFW_on_x[graph_name] = []\n",
        "    runtimes_FDFW_on_x[graph_name] = []\n",
        "\n",
        "    print(f\"Currently doing graph: {graph_name}\")\n",
        "\n",
        "    # Extract graph information\n",
        "    A = k_graph_tensor\n",
        "    Abar = complementary_hypergraph(A)\n",
        "    n = A.shape[0]\n",
        "\n",
        "    # Define regularization parameters\n",
        "    alpha = 0.45 if k == 2 else 1 / (k * (k - 1))\n",
        "    beta = n**(-k) / 2\n",
        "\n",
        "    # Define objective functions and gradients\n",
        "    objective = lambda x, y: objective_function(Abar, x, y, k, k_factorial, alpha, beta)\n",
        "    gradient_fx = lambda x, y: objective_function_gradient_x(Abar, x, y, k, k_factorial, alpha)\n",
        "    reduced_gradient_fy = lambda x, y: objective_function_reduced_gradient_y(Abar, x, y, k_factorial, beta)\n",
        "\n",
        "    objective_no_y = lambda x: objective_function(Abar, x, torch.zeros(A.shape), k, k_factorial, alpha, beta)\n",
        "    gradient_fx_no_y = lambda x: objective_function_gradient_x(Abar, x, torch.zeros(A.shape), k, k_factorial, alpha)\n",
        "\n",
        "    for i in range(max_number_sampling):\n",
        "        # Initialize x0 randomly on the simplex\n",
        "        x0 = torch.tensor(np.random.dirichlet(np.ones(n)), dtype=torch.float64)\n",
        "\n",
        "        # Initialize y0 as zeroes\n",
        "        y0 = torch.zeros(A.shape)\n",
        "\n",
        "        # Run the FWdc optimization\n",
        "        # Stop the restarts if the maximum allowed time has been hit\n",
        "        if time_counter_FWdc < max_time_allowed_in_seconds:\n",
        "            x_opt, y_opt, runtime = FWdc_SSC(objective, gradient_fx, reduced_gradient_fy, A, Abar, x0, y0, L0, n, s, k_factorial, stop_updating_y_after, max_iter, tol, verbosity=0)\n",
        "            time_counter_FWdc += runtime\n",
        "\n",
        "            if is_s_defective_clique(A, x_opt, s):\n",
        "                # Calculate the size of the support of x\n",
        "                support_size = torch.sum(x_opt > 0).item()\n",
        "\n",
        "                # Store the results\n",
        "                support_sizes_FWdc[graph_name].append(support_size)\n",
        "                runtimes_FWdc[graph_name].append(runtime)\n",
        "            else:\n",
        "                print(\"FWdc did not converge.\")\n",
        "\n",
        "        # Run the FDFW optimization\n",
        "        # Stop the restarts if the maximum allowed time has been hit\n",
        "        if time_counter_FDFW < max_time_allowed_in_seconds:\n",
        "            x_opt, y_opt, runtime = FDFW_SSC(objective, gradient_fx, reduced_gradient_fy, A, Abar, x0, y0, L0, n, s, k_factorial, max_iter, tol, verbosity=0)\n",
        "            time_counter_FDFW += runtime\n",
        "\n",
        "            if is_s_defective_clique(A, x_opt, s):\n",
        "                # Calculate the size of the support of x\n",
        "                support_size = torch.sum(x_opt > 0).item()\n",
        "\n",
        "                # Store the results\n",
        "                support_sizes_FDFW[graph_name].append(support_size)\n",
        "                runtimes_FDFW[graph_name].append(runtime)\n",
        "            else:\n",
        "                print(\"FDFW did not converge.\")\n",
        "\n",
        "        # Run the FDFW on x optimization\n",
        "        # Stop the restarts if the maximum allowed time has been hit\n",
        "        if time_counter_FDFW_on_x < max_time_allowed_in_seconds:\n",
        "            x_opt, runtime = FDFW_only_x_SSC(objective_no_y, gradient_fx_no_y, A, x0, L0, n, s, k_factorial, max_iter, tol, verbosity=0)\n",
        "            time_counter_FDFW_on_x += runtime\n",
        "\n",
        "            if is_s_defective_clique(A, x_opt, s):\n",
        "                # Calculate the size of the support of x\n",
        "                support_size = torch.sum(x_opt > 0).item()\n",
        "\n",
        "                # Store the results\n",
        "                support_sizes_FDFW_on_x[graph_name].append(support_size)\n",
        "                runtimes_FDFW_on_x[graph_name].append(runtime)\n",
        "            else:\n",
        "                print(\"FDFW on x did not converge.\")\n",
        "\n",
        "# Create a new dictionary for the normalized clique sizes\n",
        "normalized_support_sizes_FWdc = {}\n",
        "normalized_support_sizes_FDFW = {}\n",
        "normalized_support_sizes_FDFW_on_x = {}\n",
        "\n",
        "# Iterate through the original dictionary\n",
        "for key, values in support_sizes_FWdc.items():\n",
        "    normalized_support_sizes_FWdc[key] = [value/best_known_cliques_sizes[key] for value in support_sizes_FWdc[key]]\n",
        "    normalized_support_sizes_FDFW[key] = [value/best_known_cliques_sizes[key] for value in support_sizes_FDFW[key]]\n",
        "    normalized_support_sizes_FDFW_on_x[key] = [value/best_known_cliques_sizes[key] for value in support_sizes_FDFW_on_x[key]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQXgFbS9a6NO"
      },
      "outputs": [],
      "source": [
        "# Plot the normalized sizes of the cliques found for each value of s\n",
        "box_and_whisker_plot(\n",
        "    (normalized_support_sizes_FWdc, normalized_support_sizes_FDFW, normalized_support_sizes_FDFW_on_x),\n",
        "    [\"s\", \"Ratio (Best found s-defective clique size) \\n / (Best known clique size)\"],\n",
        "    [\"FWdc\", \"FDFW\", \"FDFW on x\"],\n",
        "    title=\"Normalized sizes of the s-defective \\n cliques found for each graph (k = %i, s = %i)\" % (k, s)\n",
        ")\n",
        "\n",
        "\n",
        "# Plot the runtimes for each graph (log scale)\n",
        "box_and_whisker_plot(\n",
        "     (runtimes_FWdc, runtimes_FDFW, runtimes_FDFW_on_x),\n",
        "     [\"Graph\", \"Runtime (in seconds)\"],\n",
        "     [\"FWdc\", \"FDFW\", \"FDFW on x\"],\n",
        "     y_scale='log',\n",
        "     title=\"Runtimes for each graph (k = %i, s = %i)\" % (k, s)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BtUkHVGddYW"
      },
      "source": [
        "# Application to the DIMACS dataset (varying $s$)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgseJyV4Bp0n"
      },
      "source": [
        "The following part compares the FWdc, the full FDFW and the FDFW on x with varying values for s. For each graph, only the best found s-defective clique size is kept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RkaYt9QBoNG"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "k = 3  # Number of vertices in an edge\n",
        "\n",
        "# Parameters\n",
        "graph_names = [\n",
        "    \"C125.9\", \"C250.9\", \"brock200_2\", \"brock200_4\",\n",
        "    \"keller4\", \"p_hat300-1\", \"p_hat300-2\", \"p_hat300-3\"\n",
        "]  # List of graph names on which the two formulations will be tested\n",
        "\n",
        "s_values = np.arange(1, 5)  # List of values of s on which the two algorithms will be tested\n",
        "\n",
        "L0 = 1  # Initial value for the approximation of the Lipschitz constant in the SSC\n",
        "max_number_sampling = 100  # Maximum number of restarts for each algorithm on each graph\n",
        "max_time_allowed_in_seconds = 3600  # Maximum time allowed for the restarts, finishes the last execution and stops further restarts\n",
        "stop_updating_y_after = None  # Skips the update of y after a certain number of iterations with no changes, set to None to perform all updates\n",
        "max_iter = np.inf  # Max number of iterations for each algorithm\n",
        "tol = np.inf  # Tolerance for convergence, if np.inf then the algorithm stops as soon as an s-defective clique is identified\n",
        "\n",
        "# Preprocessing\n",
        "k_factorial = math.factorial(k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMmxtlxjVVon"
      },
      "outputs": [],
      "source": [
        "# Call the function to download and convert the graphs\n",
        "graph_data = download_and_convert_DIMACS_graphs(graph_names, k)\n",
        "\n",
        "# Initialize data storage\n",
        "support_sizes_FWdc = {}\n",
        "runtimes_FWdc = {}\n",
        "\n",
        "support_sizes_FDFW = {}\n",
        "runtimes_FDFW = {}\n",
        "\n",
        "support_sizes_FDFW_on_x = {}\n",
        "runtimes_FDFW_on_x = {}\n",
        "\n",
        "# Run random initializations on each graph for each value of s\n",
        "for s_value in s_values:\n",
        "    print(f\"Currently doing s = {s_value}\")\n",
        "\n",
        "    # Initialize dictionaries to store results for each graph\n",
        "    support_sizes_FWdc[s_value] = {}\n",
        "    support_sizes_FDFW[s_value] = {}\n",
        "    support_sizes_FDFW_on_x[s_value] = {}\n",
        "\n",
        "    runtimes_FWdc[s_value] = {}\n",
        "    runtimes_FDFW[s_value] = {}\n",
        "    runtimes_FDFW_on_x[s_value] = {}\n",
        "\n",
        "    # Access the k-graph tensor for each graph\n",
        "    for graph_name, k_graph_tensor in graph_data.items():\n",
        "        # Initialize the time counters\n",
        "        time_counter_FWdc = 0\n",
        "        time_counter_FDFW = 0\n",
        "        time_counter_FDFW_on_x = 0\n",
        "\n",
        "        # Initialize the time counter\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Initialize values and dictionaries to store results for each graph\n",
        "        support_sizes_FWdc[s_value][graph_name] = []\n",
        "        support_sizes_FDFW[s_value][graph_name] = []\n",
        "        support_sizes_FDFW_on_x[s_value][graph_name] = []\n",
        "\n",
        "        runtimes_FWdc[s_value][graph_name] = []\n",
        "        runtimes_FDFW[s_value][graph_name] = []\n",
        "        runtimes_FDFW_on_x[s_value][graph_name] = []\n",
        "\n",
        "        # Extract graph information\n",
        "        A = k_graph_tensor\n",
        "        Abar = complementary_hypergraph(A)\n",
        "        n = A.shape[0]\n",
        "\n",
        "        # Define regularization parameters\n",
        "        alpha = 0.45 if k == 2 else 1 / (k * (k - 1))\n",
        "        beta = n**(-k) / 2\n",
        "\n",
        "        # Define objective functions and gradients\n",
        "        objective = lambda x, y: objective_function(Abar, x, y, k, k_factorial, alpha, beta)\n",
        "        gradient_fx = lambda x, y: objective_function_gradient_x(Abar, x, y, k, k_factorial, alpha)\n",
        "        reduced_gradient_fy = lambda x, y: objective_function_reduced_gradient_y(Abar, x, y, k_factorial, beta)\n",
        "\n",
        "        objective_no_y = lambda x: objective_function(Abar, x, torch.zeros(A.shape), k, k_factorial, alpha, beta)\n",
        "        gradient_fx_no_y = lambda x: objective_function_gradient_x(Abar, x, torch.zeros(A.shape), k, k_factorial, alpha)\n",
        "\n",
        "        for i in range(max_number_sampling):\n",
        "            # Initialize x0 randomly on the simplex\n",
        "            x0 = torch.tensor(np.random.dirichlet(np.ones(n)), dtype=torch.float64)\n",
        "\n",
        "            # Initialize y0 as zeroes\n",
        "            y0 = torch.zeros(A.shape)\n",
        "\n",
        "            # Run the FWdc optimization\n",
        "            # Stop the restarts if the maximum allowed time has been hit\n",
        "            if time_counter_FWdc < max_time_allowed_in_seconds:\n",
        "                x_opt, y_opt, runtime = FWdc_SSC(objective, gradient_fx, reduced_gradient_fy, A, Abar, x0, y0, L0, n, s_value, k_factorial, stop_updating_y_after, max_iter, tol, verbosity=0)\n",
        "                time_counter_FWdc += runtime\n",
        "\n",
        "                if is_s_defective_clique(A, x_opt, s_value):\n",
        "                    # Calculate the size of the support of x\n",
        "                    support_size = torch.sum(x_opt > 0).item()\n",
        "\n",
        "                    # Store the results\n",
        "                    support_sizes_FWdc[s_value][graph_name].append(support_size)\n",
        "                    runtimes_FWdc[s_value][graph_name].append(runtime)\n",
        "                else:\n",
        "                    print(\"FWdc did not converge.\")\n",
        "\n",
        "            # Run the FDFW optimization\n",
        "            # Stop the restarts if the maximum allowed time has been hit\n",
        "            if time_counter_FDFW < max_time_allowed_in_seconds:\n",
        "                x_opt, y_opt, runtime = FDFW_SSC(objective, gradient_fx, reduced_gradient_fy, A, Abar, x0, y0, L0, n, s_value, k_factorial, max_iter, tol, verbosity=0)\n",
        "                time_counter_FDFW += runtime\n",
        "\n",
        "                if is_s_defective_clique(A, x_opt, s_value):\n",
        "                    # Calculate the size of the support of x\n",
        "                    support_size = torch.sum(x_opt > 0).item()\n",
        "\n",
        "                    # Store the results\n",
        "                    support_sizes_FDFW[s_value][graph_name].append(support_size)\n",
        "                    runtimes_FDFW[s_value][graph_name].append(runtime)\n",
        "                else:\n",
        "                    print(\"FDFW did not converge.\")\n",
        "\n",
        "            # Run the FDFW on x optimization\n",
        "            # Stop the restarts if the maximum allowed time has been hit\n",
        "            if time_counter_FDFW_on_x < max_time_allowed_in_seconds:\n",
        "                x_opt, runtime = FDFW_only_x_SSC(objective_no_y, gradient_fx_no_y, A, x0, L0, n, s_value, k_factorial, max_iter, tol, verbosity=0)\n",
        "                time_counter_FDFW_on_x += runtime\n",
        "\n",
        "                if is_s_defective_clique(A, x_opt, s_value):\n",
        "                    # Calculate the size of the support of x\n",
        "                    support_size = torch.sum(x_opt > 0).item()\n",
        "\n",
        "                    # Store the results\n",
        "                    support_sizes_FDFW_on_x[s_value][graph_name].append(support_size)\n",
        "                    runtimes_FDFW_on_x[s_value][graph_name].append(runtime)\n",
        "                else:\n",
        "                    print(\"FDFW on x did not converge.\")\n",
        "\n",
        "# Store the maximum clique sizes found\n",
        "normalized_support_sizes_FWdc = {\n",
        "    str(s_value): [np.max(support_sizes_FWdc[s_value][graph_name]) / best_known_cliques_sizes[graph_name] for graph_name in graph_names] for s_value in s_values\n",
        "}\n",
        "normalized_support_sizes_FDFW = {\n",
        "    str(s_value): [np.max(support_sizes_FDFW[s_value][graph_name]) / best_known_cliques_sizes[graph_name] for graph_name in graph_names] for s_value in s_values\n",
        "}\n",
        "normalized_support_sizes_FDFW_on_x = {\n",
        "    str(s_value): [np.max(support_sizes_FDFW_on_x[s_value][graph_name]) / best_known_cliques_sizes[graph_name] for graph_name in graph_names] for s_value in s_values\n",
        "}\n",
        "\n",
        "# Store the runtimes in a dictionary by value of s\n",
        "runtimes_FWdc_by_s = {str(s_value) : np.concatenate(runtimes_FWdc[s_value]) for s_value in s_values}\n",
        "runtimes_FDFW_by_s = {str(s_value) : np.concatenate(runtimes_FDFW[s_value]) for s_value in s_values}\n",
        "runtimes_FDFW_on_x_by_s = {str(s_value) : np.concatenate(runtimes_FDFW_on_x[s_value]) for s_value in s_values}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96rp9U4mhutw"
      },
      "outputs": [],
      "source": [
        "# Plot the normalized sizes of the cliques found for each value of s\n",
        "box_and_whisker_plot(\n",
        "    (normalized_support_sizes_FWdc, normalized_support_sizes_FDFW, normalized_support_sizes_FDFW_on_x),\n",
        "    [\"s\", \"Ratio (Best found s-defective clique size) \\n / (Best known clique size)\"],\n",
        "    [\"FWdc\", \"FDFW\", \"FDFW on x\"],\n",
        "    title=\"Normalized sizes of the s-defective \\n cliques found for each value of s (k = %i)\" % k\n",
        ")\n",
        "\n",
        "\n",
        "# Plot the runtimes for each graph (log scale)\n",
        "box_and_whisker_plot(\n",
        "     (runtimes_FWdc_by_s, runtimes_FDFW_by_s, runtimes_FDFW_on_x_by_s),\n",
        "     [\"Graph\", \"Runtime (in seconds)\"],\n",
        "     [\"FWdc\", \"FDFW\", \"FDFW on x\"],\n",
        "     y_scale='log',\n",
        "     title=\"Runtimes for each graph (k = %i)\" % k\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "xfK7rGJE-Isj",
        "-CxjZLot-Sj_",
        "m8GWG1hZCHT-",
        "TA7PNzM7-Yc1",
        "Qv5InhDD-doe",
        "Uo3hORAJ1fqL",
        "Pg81Z8tX-iVK",
        "S1mHAsno-rPv",
        "00D9meekxIHT",
        "USMFoUbOFm72",
        "DHuZ_4nDdlWF",
        "obf1MftwJBzb",
        "9BtUkHVGddYW"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}